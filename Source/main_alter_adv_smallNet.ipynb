{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 32, 32, 3)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from head.data_utils import load_CIFAR10\n",
    "\n",
    "\n",
    "#Load Data\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    #mean_image = np.mean(X_train, axis=0)\n",
    "    #X_train -= mean_image\n",
    "    #X_val -= mean_image\n",
    "    #X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    #X_train = X_train.reshape(num_training, -1)\n",
    "    #X_val = X_val.reshape(num_validation, -1)\n",
    "    #X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 768)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 768)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 768)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from head.data_utils import load_CIFAR10\n",
    "\n",
    "\n",
    "#Load Data\n",
    "\n",
    "def get_CIFAR10_data_downsample(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask, 0:31:2, 0:31:2]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask, 0:31:2, 0:31:2]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask, 0:31:2, 0:31:2]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data_downsample()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cb9a8a6d0582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAACGCAYAAAAxUs9TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2107ed57b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#X_show = X_train.reshape(49000, 32, 32, 3)\n",
    "#plt.figure(figsize = (15, 15))\n",
    "for i in np.arange(1, 100, 10):\n",
    "    plt.subplot(2, 5, int(i//10)+1)\n",
    "    plt.imshow(X_train[i, 0:31:2, 0:31:2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network Settings\n",
    "\n",
    "input_size = 16 * 16 * 3\n",
    "hidden_size = 30\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: loss 2.302649\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "9222 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\CBICR_contest\\Source\\head\\neural_net.py:291: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 3000: loss 2.306411\n",
      "Ratio: 1.001847722843567   Reject: 0   Accept: 101\n",
      "733 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 200 / 3000: loss 2.310198\n",
      "Ratio: 0.9992520610480763   Reject: 0   Accept: 201\n",
      "18 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 300 / 3000: loss 2.313246\n",
      "Ratio: 0.9949382361041582   Reject: 0   Accept: 301\n",
      "9 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 400 / 3000: loss 2.314400\n",
      "Ratio: 0.9922209228142639   Reject: 2   Accept: 399\n",
      "122 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 500 / 3000: loss 2.317292\n",
      "Ratio: 0.9924781892084378   Reject: 2   Accept: 499\n",
      "769 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 600 / 3000: loss 2.311828\n",
      "Ratio: 1.0437733520842125   Reject: 4   Accept: 597\n",
      "108 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 700 / 3000: loss 2.319848\n",
      "Ratio: 0.9676487629796174   Reject: 6   Accept: 695\n",
      "23040 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 800 / 3000: loss 2.314717\n",
      "Ratio: 1.0377061041515394   Reject: 8   Accept: 793\n",
      "10 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 900 / 3000: loss 2.316648\n",
      "Ratio: 0.971040490806696   Reject: 10   Accept: 891\n",
      "23040 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1000 / 3000: loss 2.323999\n",
      "Ratio: 0.8722681313345337   Reject: 13   Accept: 988\n",
      "83 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 1100 / 3000: loss 2.312688\n",
      "Ratio: 1.0181583859067842   Reject: 16   Accept: 1085\n",
      "13 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1200 / 3000: loss 2.321073\n",
      "Ratio: 0.9654093977454213   Reject: 25   Accept: 1176\n",
      "23040 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1300 / 3000: loss 2.319709\n",
      "Ratio: 0.8223874446124085   Reject: 27   Accept: 1274\n",
      "10368 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1400 / 3000: loss 2.317616\n",
      "Ratio: 1.0889240826787383   Reject: 28   Accept: 1373\n",
      "1395 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 1500 / 3000: loss 2.319654\n",
      "Ratio: 0.7470993040360002   Reject: 32   Accept: 1469\n",
      "487 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 1600 / 3000: loss 2.316925\n",
      "Ratio: 0.9408224679659074   Reject: 34   Accept: 1567\n",
      "41 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1700 / 3000: loss 2.313586\n",
      "Ratio: 1.2910521637794417   Reject: 46   Accept: 1655\n",
      "23040 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 1800 / 3000: loss 2.317334\n",
      "Ratio: 1.117765408473709   Reject: 53   Accept: 1748\n",
      "23040 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 1900 / 3000: loss 2.309950\n",
      "Ratio: 2.7203394264784153   Reject: 65   Accept: 1836\n",
      "4649 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 2000 / 3000: loss 2.313168\n",
      "Ratio: 0.9350330380235669   Reject: 84   Accept: 1917\n",
      "2440 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 2100 / 3000: loss 2.312508\n",
      "Ratio: 0.47139247865376643   Reject: 108   Accept: 1993\n",
      "53 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 2200 / 3000: loss 2.313687\n",
      "Ratio: 1.4203280959231206   Reject: 123   Accept: 2078\n",
      "95 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 2300 / 3000: loss 2.315156\n",
      "Ratio: 0.7381025735289479   Reject: 154   Accept: 2147\n",
      "6165 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 2400 / 3000: loss 2.315200\n",
      "Ratio: 0.25736895273096383   Reject: 200   Accept: 2201\n",
      "42 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 2500 / 3000: loss 2.309129\n",
      "Ratio: 0.1830616774715614   Reject: 241   Accept: 2260\n",
      "7285 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 2600 / 3000: loss 2.310088\n",
      "Ratio: 0.052925699827578776   Reject: 308   Accept: 2293\n",
      "80 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 2700 / 3000: loss 2.313554\n",
      "Ratio: 0.9071465502800247   Reject: 356   Accept: 2345\n",
      "13 / 23040 W1 params change\n",
      "Punish for W1 mean 0.000000, max 0.000000\n",
      "iteration 2800 / 3000: loss 2.313066\n",
      "Ratio: 0.3365538388568017   Reject: 429   Accept: 2372\n",
      "1197 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "iteration 2900 / 3000: loss 2.310386\n",
      "Ratio: 0.006617061622435055   Reject: 498   Accept: 2403\n",
      "120 / 23040 W1 params change\n",
      "Punish for W1 mean -0.000000, max 0.000000\n",
      "Test accuracy:  0.068\n"
     ]
    }
   ],
   "source": [
    "#Training hyperparameters\n",
    "batch_size = 200\n",
    "step_len = 0.001\n",
    "reg = 0.1\n",
    "punish_strength = 0.01\n",
    "if_sparse = True\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_sa = net_sa.train_sa_sparse(X_train, y_train, X_val, y_val,\n",
    "        num_iters=3000, batch_size=batch_size, step_len = step_len,\n",
    "        reg=reg, T_max = 0.5, T_min = 0.001, if_sparse = if_sparse,\n",
    "        punish_strength = punish_strength, verbose=True)\n",
    "\n",
    "test_acc = (net_sa.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return to alternative training approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 th training\n",
      "iteration 0 / 20000: loss 2.302604\n",
      "iteration 1000 / 20000: loss 1.721576\n",
      "iteration 2000 / 20000: loss 1.619782\n",
      "iteration 3000 / 20000: loss 1.617149\n",
      "iteration 4000 / 20000: loss 1.588005\n",
      "iteration 5000 / 20000: loss 1.476857\n",
      "iteration 6000 / 20000: loss 1.416130\n",
      "iteration 7000 / 20000: loss 1.400155\n",
      "iteration 8000 / 20000: loss 1.382873\n",
      "iteration 9000 / 20000: loss 1.396611\n",
      "iteration 10000 / 20000: loss 1.447586\n",
      "iteration 11000 / 20000: loss 1.541459\n",
      "iteration 12000 / 20000: loss 1.486562\n",
      "iteration 13000 / 20000: loss 1.491178\n",
      "iteration 14000 / 20000: loss 1.590341\n",
      "iteration 15000 / 20000: loss 1.459033\n",
      "iteration 16000 / 20000: loss 1.412552\n",
      "iteration 17000 / 20000: loss 1.364793\n",
      "iteration 18000 / 20000: loss 1.607943\n",
      "iteration 19000 / 20000: loss 1.467657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\CBICR_contest\\Source\\head\\neural_net.py:180: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: loss 1.454242\n",
      "iteration 0 / 3000: train_acc 0.535000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.869075\n",
      "iteration 1000 / 3000: train_acc 0.365000\n",
      "Reject: 310   Accept: 691\n",
      "iteration 2000 / 3000: loss 1.569035\n",
      "iteration 2000 / 3000: train_acc 0.430000\n",
      "Reject: 1211   Accept: 790\n",
      "iteration 0 / 20000: loss 1.812077\n",
      "iteration 1000 / 20000: loss 1.462782\n",
      "iteration 2000 / 20000: loss 1.639652\n",
      "iteration 3000 / 20000: loss 1.449144\n",
      "iteration 4000 / 20000: loss 1.386403\n",
      "iteration 5000 / 20000: loss 1.473546\n",
      "iteration 6000 / 20000: loss 1.375544\n",
      "iteration 7000 / 20000: loss 1.459917\n",
      "iteration 8000 / 20000: loss 1.466495\n",
      "iteration 9000 / 20000: loss 1.511165\n",
      "iteration 10000 / 20000: loss 1.520412\n",
      "iteration 11000 / 20000: loss 1.363248\n",
      "iteration 12000 / 20000: loss 1.347115\n",
      "iteration 13000 / 20000: loss 1.547290\n",
      "iteration 14000 / 20000: loss 1.379956\n",
      "iteration 15000 / 20000: loss 1.424331\n",
      "iteration 16000 / 20000: loss 1.388433\n",
      "iteration 17000 / 20000: loss 1.398023\n",
      "iteration 18000 / 20000: loss 1.404411\n",
      "iteration 19000 / 20000: loss 1.503459\n",
      "Train accuracy:  0.5301829268292684\n",
      "Loss:  1.4306362893610611\n",
      "Test accuracy:  0.48\n",
      " \n",
      "The 1 th training\n",
      "iteration 0 / 20000: loss 2.302603\n",
      "iteration 1000 / 20000: loss 1.814850\n",
      "iteration 2000 / 20000: loss 1.571370\n",
      "iteration 3000 / 20000: loss 1.609560\n",
      "iteration 4000 / 20000: loss 1.534604\n",
      "iteration 5000 / 20000: loss 1.558490\n",
      "iteration 6000 / 20000: loss 1.505132\n",
      "iteration 7000 / 20000: loss 1.480086\n",
      "iteration 8000 / 20000: loss 1.637630\n",
      "iteration 9000 / 20000: loss 1.536770\n",
      "iteration 10000 / 20000: loss 1.463960\n",
      "iteration 11000 / 20000: loss 1.540391\n",
      "iteration 12000 / 20000: loss 1.316838\n",
      "iteration 13000 / 20000: loss 1.464574\n",
      "iteration 14000 / 20000: loss 1.423300\n",
      "iteration 15000 / 20000: loss 1.382473\n",
      "iteration 16000 / 20000: loss 1.554756\n",
      "iteration 17000 / 20000: loss 1.565275\n",
      "iteration 18000 / 20000: loss 1.475893\n",
      "iteration 19000 / 20000: loss 1.446563\n",
      "iteration 0 / 3000: loss 1.394656\n",
      "iteration 0 / 3000: train_acc 0.565000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.428717\n",
      "iteration 1000 / 3000: train_acc 0.455000\n",
      "Reject: 299   Accept: 702\n",
      "iteration 2000 / 3000: loss 1.493484\n",
      "iteration 2000 / 3000: train_acc 0.370000\n",
      "Reject: 1239   Accept: 762\n",
      "iteration 0 / 20000: loss 1.704179\n",
      "iteration 1000 / 20000: loss 1.439858\n",
      "iteration 2000 / 20000: loss 1.417105\n",
      "iteration 3000 / 20000: loss 1.439080\n",
      "iteration 4000 / 20000: loss 1.526891\n",
      "iteration 5000 / 20000: loss 1.609251\n",
      "iteration 6000 / 20000: loss 1.414901\n",
      "iteration 7000 / 20000: loss 1.533824\n",
      "iteration 8000 / 20000: loss 1.447301\n",
      "iteration 9000 / 20000: loss 1.398172\n",
      "iteration 10000 / 20000: loss 1.309665\n",
      "iteration 11000 / 20000: loss 1.355048\n",
      "iteration 12000 / 20000: loss 1.414573\n",
      "iteration 13000 / 20000: loss 1.431614\n",
      "iteration 14000 / 20000: loss 1.422626\n",
      "iteration 15000 / 20000: loss 1.314737\n",
      "iteration 16000 / 20000: loss 1.508642\n",
      "iteration 17000 / 20000: loss 1.524636\n",
      "iteration 18000 / 20000: loss 1.417694\n",
      "iteration 19000 / 20000: loss 1.509065\n",
      "Train accuracy:  0.530609756097561\n",
      "Loss:  1.4323192488022127\n",
      "Test accuracy:  0.489\n",
      " \n",
      "The 2 th training\n",
      "iteration 0 / 20000: loss 2.302604\n",
      "iteration 1000 / 20000: loss 1.899047\n",
      "iteration 2000 / 20000: loss 1.721879\n",
      "iteration 3000 / 20000: loss 1.607627\n",
      "iteration 4000 / 20000: loss 1.449211\n",
      "iteration 5000 / 20000: loss 1.512542\n",
      "iteration 6000 / 20000: loss 1.539552\n",
      "iteration 7000 / 20000: loss 1.477220\n",
      "iteration 8000 / 20000: loss 1.389212\n",
      "iteration 9000 / 20000: loss 1.440825\n",
      "iteration 10000 / 20000: loss 1.525575\n",
      "iteration 11000 / 20000: loss 1.583101\n",
      "iteration 12000 / 20000: loss 1.593108\n",
      "iteration 13000 / 20000: loss 1.403422\n",
      "iteration 14000 / 20000: loss 1.436041\n",
      "iteration 15000 / 20000: loss 1.499716\n",
      "iteration 16000 / 20000: loss 1.432628\n",
      "iteration 17000 / 20000: loss 1.491039\n",
      "iteration 18000 / 20000: loss 1.557108\n",
      "iteration 19000 / 20000: loss 1.497400\n",
      "iteration 0 / 3000: loss 1.356161\n",
      "iteration 0 / 3000: train_acc 0.565000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.592081\n",
      "iteration 1000 / 3000: train_acc 0.405000\n",
      "Reject: 297   Accept: 704\n",
      "iteration 2000 / 3000: loss 1.588492\n",
      "iteration 2000 / 3000: train_acc 0.355000\n",
      "Reject: 1168   Accept: 833\n",
      "iteration 0 / 20000: loss 1.801487\n",
      "iteration 1000 / 20000: loss 1.555309\n",
      "iteration 2000 / 20000: loss 1.593366\n",
      "iteration 3000 / 20000: loss 1.484054\n",
      "iteration 4000 / 20000: loss 1.535378\n",
      "iteration 5000 / 20000: loss 1.344736\n",
      "iteration 6000 / 20000: loss 1.435962\n",
      "iteration 7000 / 20000: loss 1.482803\n",
      "iteration 8000 / 20000: loss 1.415899\n",
      "iteration 9000 / 20000: loss 1.351754\n",
      "iteration 10000 / 20000: loss 1.407453\n",
      "iteration 11000 / 20000: loss 1.569490\n",
      "iteration 12000 / 20000: loss 1.344365\n",
      "iteration 13000 / 20000: loss 1.447909\n",
      "iteration 14000 / 20000: loss 1.508478\n",
      "iteration 15000 / 20000: loss 1.420218\n",
      "iteration 16000 / 20000: loss 1.360990\n",
      "iteration 17000 / 20000: loss 1.414993\n",
      "iteration 18000 / 20000: loss 1.421422\n",
      "iteration 19000 / 20000: loss 1.445400\n",
      "Train accuracy:  0.5320121951219512\n",
      "Loss:  1.4298028966352931\n",
      "Test accuracy:  0.472\n",
      " \n",
      "The 3 th training\n",
      "iteration 0 / 20000: loss 2.302611\n",
      "iteration 1000 / 20000: loss 1.817359\n",
      "iteration 2000 / 20000: loss 1.699524\n",
      "iteration 3000 / 20000: loss 1.626222\n",
      "iteration 4000 / 20000: loss 1.604746\n",
      "iteration 5000 / 20000: loss 1.568621\n",
      "iteration 6000 / 20000: loss 1.462498\n",
      "iteration 7000 / 20000: loss 1.536009\n",
      "iteration 8000 / 20000: loss 1.523336\n",
      "iteration 9000 / 20000: loss 1.514383\n",
      "iteration 10000 / 20000: loss 1.431194\n",
      "iteration 11000 / 20000: loss 1.554812\n",
      "iteration 12000 / 20000: loss 1.423541\n",
      "iteration 13000 / 20000: loss 1.317049\n",
      "iteration 14000 / 20000: loss 1.471456\n",
      "iteration 15000 / 20000: loss 1.501074\n",
      "iteration 16000 / 20000: loss 1.536414\n",
      "iteration 17000 / 20000: loss 1.369667\n",
      "iteration 18000 / 20000: loss 1.476146\n",
      "iteration 19000 / 20000: loss 1.445419\n",
      "iteration 0 / 3000: loss 1.496588\n",
      "iteration 0 / 3000: train_acc 0.520000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.543404\n",
      "iteration 1000 / 3000: train_acc 0.515000\n",
      "Reject: 301   Accept: 700\n",
      "iteration 2000 / 3000: loss 1.473239\n",
      "iteration 2000 / 3000: train_acc 0.420000\n",
      "Reject: 1227   Accept: 774\n",
      "iteration 0 / 20000: loss 1.785575\n",
      "iteration 1000 / 20000: loss 1.521152\n",
      "iteration 2000 / 20000: loss 1.422321\n",
      "iteration 3000 / 20000: loss 1.457241\n",
      "iteration 4000 / 20000: loss 1.517654\n",
      "iteration 5000 / 20000: loss 1.400651\n",
      "iteration 6000 / 20000: loss 1.377395\n",
      "iteration 7000 / 20000: loss 1.404542\n",
      "iteration 8000 / 20000: loss 1.363788\n",
      "iteration 9000 / 20000: loss 1.327350\n",
      "iteration 10000 / 20000: loss 1.409576\n",
      "iteration 11000 / 20000: loss 1.322381\n",
      "iteration 12000 / 20000: loss 1.542855\n",
      "iteration 13000 / 20000: loss 1.388821\n",
      "iteration 14000 / 20000: loss 1.477159\n",
      "iteration 15000 / 20000: loss 1.525191\n",
      "iteration 16000 / 20000: loss 1.421957\n",
      "iteration 17000 / 20000: loss 1.450107\n",
      "iteration 18000 / 20000: loss 1.416567\n",
      "iteration 19000 / 20000: loss 1.376231\n",
      "Train accuracy:  0.5333536585365853\n",
      "Loss:  1.4280449913421083\n",
      "Test accuracy:  0.477\n",
      " \n",
      "The 4 th training\n",
      "iteration 0 / 20000: loss 2.302626\n",
      "iteration 1000 / 20000: loss 1.790901\n",
      "iteration 2000 / 20000: loss 1.610478\n",
      "iteration 3000 / 20000: loss 1.621045\n",
      "iteration 4000 / 20000: loss 1.559380\n",
      "iteration 5000 / 20000: loss 1.492451\n",
      "iteration 6000 / 20000: loss 1.478557\n",
      "iteration 7000 / 20000: loss 1.339867\n",
      "iteration 8000 / 20000: loss 1.548012\n",
      "iteration 9000 / 20000: loss 1.533423\n",
      "iteration 10000 / 20000: loss 1.552505\n",
      "iteration 11000 / 20000: loss 1.469270\n",
      "iteration 12000 / 20000: loss 1.403011\n",
      "iteration 13000 / 20000: loss 1.543505\n",
      "iteration 14000 / 20000: loss 1.403159\n",
      "iteration 15000 / 20000: loss 1.480682\n",
      "iteration 16000 / 20000: loss 1.440431\n",
      "iteration 17000 / 20000: loss 1.455398\n",
      "iteration 18000 / 20000: loss 1.521364\n",
      "iteration 19000 / 20000: loss 1.474712\n",
      "iteration 0 / 3000: loss 1.347806\n",
      "iteration 0 / 3000: train_acc 0.535000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.599963\n",
      "iteration 1000 / 3000: train_acc 0.460000\n",
      "Reject: 320   Accept: 681\n",
      "iteration 2000 / 3000: loss 1.557797\n",
      "iteration 2000 / 3000: train_acc 0.420000\n",
      "Reject: 1229   Accept: 772\n",
      "iteration 0 / 20000: loss 1.767858\n",
      "iteration 1000 / 20000: loss 1.410069\n",
      "iteration 2000 / 20000: loss 1.462228\n",
      "iteration 3000 / 20000: loss 1.474606\n",
      "iteration 4000 / 20000: loss 1.452297\n",
      "iteration 5000 / 20000: loss 1.355432\n",
      "iteration 6000 / 20000: loss 1.508528\n",
      "iteration 7000 / 20000: loss 1.503141\n",
      "iteration 8000 / 20000: loss 1.445114\n",
      "iteration 9000 / 20000: loss 1.442034\n",
      "iteration 10000 / 20000: loss 1.427786\n",
      "iteration 11000 / 20000: loss 1.533375\n",
      "iteration 12000 / 20000: loss 1.433398\n",
      "iteration 13000 / 20000: loss 1.516030\n",
      "iteration 14000 / 20000: loss 1.488625\n",
      "iteration 15000 / 20000: loss 1.449787\n",
      "iteration 16000 / 20000: loss 1.469477\n",
      "iteration 17000 / 20000: loss 1.317003\n",
      "iteration 18000 / 20000: loss 1.508598\n",
      "iteration 19000 / 20000: loss 1.406662\n",
      "Train accuracy:  0.5287195121951219\n",
      "Loss:  1.4287240507714323\n",
      "Test accuracy:  0.491\n",
      " \n",
      "The 5 th training\n",
      "iteration 0 / 20000: loss 2.302613\n",
      "iteration 1000 / 20000: loss 1.699647\n",
      "iteration 2000 / 20000: loss 1.655378\n",
      "iteration 3000 / 20000: loss 1.607700\n",
      "iteration 4000 / 20000: loss 1.444048\n",
      "iteration 5000 / 20000: loss 1.516519\n",
      "iteration 6000 / 20000: loss 1.596063\n",
      "iteration 7000 / 20000: loss 1.475231\n",
      "iteration 8000 / 20000: loss 1.421070\n",
      "iteration 9000 / 20000: loss 1.450458\n",
      "iteration 10000 / 20000: loss 1.360666\n",
      "iteration 11000 / 20000: loss 1.508641\n",
      "iteration 12000 / 20000: loss 1.484208\n",
      "iteration 13000 / 20000: loss 1.659496\n",
      "iteration 14000 / 20000: loss 1.312599\n",
      "iteration 15000 / 20000: loss 1.431703\n",
      "iteration 16000 / 20000: loss 1.320238\n",
      "iteration 17000 / 20000: loss 1.536549\n",
      "iteration 18000 / 20000: loss 1.433591\n",
      "iteration 19000 / 20000: loss 1.538085\n",
      "iteration 0 / 3000: loss 1.629866\n",
      "iteration 0 / 3000: train_acc 0.470000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.515097\n",
      "iteration 1000 / 3000: train_acc 0.445000\n",
      "Reject: 289   Accept: 712\n",
      "iteration 2000 / 3000: loss 1.459325\n",
      "iteration 2000 / 3000: train_acc 0.450000\n",
      "Reject: 1169   Accept: 832\n",
      "iteration 0 / 20000: loss 1.715723\n",
      "iteration 1000 / 20000: loss 1.536698\n",
      "iteration 2000 / 20000: loss 1.509444\n",
      "iteration 3000 / 20000: loss 1.587105\n",
      "iteration 4000 / 20000: loss 1.401362\n",
      "iteration 5000 / 20000: loss 1.631000\n",
      "iteration 6000 / 20000: loss 1.371650\n",
      "iteration 7000 / 20000: loss 1.537562\n",
      "iteration 8000 / 20000: loss 1.387735\n",
      "iteration 9000 / 20000: loss 1.528132\n",
      "iteration 10000 / 20000: loss 1.298020\n",
      "iteration 11000 / 20000: loss 1.275275\n",
      "iteration 12000 / 20000: loss 1.376869\n",
      "iteration 13000 / 20000: loss 1.451385\n",
      "iteration 14000 / 20000: loss 1.421492\n",
      "iteration 15000 / 20000: loss 1.474308\n",
      "iteration 16000 / 20000: loss 1.566153\n",
      "iteration 17000 / 20000: loss 1.515039\n",
      "iteration 18000 / 20000: loss 1.411332\n",
      "iteration 19000 / 20000: loss 1.411539\n",
      "Train accuracy:  0.5254268292682926\n",
      "Loss:  1.4299130867247447\n",
      "Test accuracy:  0.476\n",
      " \n",
      "The 6 th training\n",
      "iteration 0 / 20000: loss 2.302599\n",
      "iteration 1000 / 20000: loss 1.749879\n",
      "iteration 2000 / 20000: loss 1.639435\n",
      "iteration 3000 / 20000: loss 1.521666\n",
      "iteration 4000 / 20000: loss 1.526821\n",
      "iteration 5000 / 20000: loss 1.462358\n",
      "iteration 6000 / 20000: loss 1.541843\n",
      "iteration 7000 / 20000: loss 1.524625\n",
      "iteration 8000 / 20000: loss 1.483391\n",
      "iteration 9000 / 20000: loss 1.540213\n",
      "iteration 10000 / 20000: loss 1.393090\n",
      "iteration 11000 / 20000: loss 1.511347\n",
      "iteration 12000 / 20000: loss 1.444817\n",
      "iteration 13000 / 20000: loss 1.456268\n",
      "iteration 14000 / 20000: loss 1.465095\n",
      "iteration 15000 / 20000: loss 1.486225\n",
      "iteration 16000 / 20000: loss 1.499469\n",
      "iteration 17000 / 20000: loss 1.474336\n",
      "iteration 18000 / 20000: loss 1.584604\n",
      "iteration 19000 / 20000: loss 1.530187\n",
      "iteration 0 / 3000: loss 1.554126\n",
      "iteration 0 / 3000: train_acc 0.490000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.690087\n",
      "iteration 1000 / 3000: train_acc 0.435000\n",
      "Reject: 272   Accept: 729\n",
      "iteration 2000 / 3000: loss 1.459948\n",
      "iteration 2000 / 3000: train_acc 0.460000\n",
      "Reject: 1160   Accept: 841\n",
      "iteration 0 / 20000: loss 1.755944\n",
      "iteration 1000 / 20000: loss 1.518646\n",
      "iteration 2000 / 20000: loss 1.485648\n",
      "iteration 3000 / 20000: loss 1.546845\n",
      "iteration 4000 / 20000: loss 1.437593\n",
      "iteration 5000 / 20000: loss 1.428161\n",
      "iteration 6000 / 20000: loss 1.357426\n",
      "iteration 7000 / 20000: loss 1.335861\n",
      "iteration 8000 / 20000: loss 1.377356\n",
      "iteration 9000 / 20000: loss 1.339168\n",
      "iteration 10000 / 20000: loss 1.419325\n",
      "iteration 11000 / 20000: loss 1.364695\n",
      "iteration 12000 / 20000: loss 1.436545\n",
      "iteration 13000 / 20000: loss 1.310827\n",
      "iteration 14000 / 20000: loss 1.275995\n",
      "iteration 15000 / 20000: loss 1.453353\n",
      "iteration 16000 / 20000: loss 1.431729\n",
      "iteration 17000 / 20000: loss 1.320017\n",
      "iteration 18000 / 20000: loss 1.369340\n",
      "iteration 19000 / 20000: loss 1.445881\n",
      "Train accuracy:  0.5245121951219512\n",
      "Loss:  1.4299512294311956\n",
      "Test accuracy:  0.501\n",
      " \n",
      "The 7 th training\n",
      "iteration 0 / 20000: loss 2.302608\n",
      "iteration 1000 / 20000: loss 1.741497\n",
      "iteration 2000 / 20000: loss 1.601935\n",
      "iteration 3000 / 20000: loss 1.578027\n",
      "iteration 4000 / 20000: loss 1.486551\n",
      "iteration 5000 / 20000: loss 1.407979\n",
      "iteration 6000 / 20000: loss 1.550502\n",
      "iteration 7000 / 20000: loss 1.428756\n",
      "iteration 8000 / 20000: loss 1.484829\n",
      "iteration 9000 / 20000: loss 1.390885\n",
      "iteration 10000 / 20000: loss 1.476092\n",
      "iteration 11000 / 20000: loss 1.527754\n",
      "iteration 12000 / 20000: loss 1.412825\n",
      "iteration 13000 / 20000: loss 1.514714\n",
      "iteration 14000 / 20000: loss 1.479507\n",
      "iteration 15000 / 20000: loss 1.474166\n",
      "iteration 16000 / 20000: loss 1.557024\n",
      "iteration 17000 / 20000: loss 1.507668\n",
      "iteration 18000 / 20000: loss 1.443590\n",
      "iteration 19000 / 20000: loss 1.404387\n",
      "iteration 0 / 3000: loss 1.493426\n",
      "iteration 0 / 3000: train_acc 0.470000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.621195\n",
      "iteration 1000 / 3000: train_acc 0.465000\n",
      "Reject: 265   Accept: 736\n",
      "iteration 2000 / 3000: loss 1.467452\n",
      "iteration 2000 / 3000: train_acc 0.415000\n",
      "Reject: 1176   Accept: 825\n",
      "iteration 0 / 20000: loss 1.558072\n",
      "iteration 1000 / 20000: loss 1.410448\n",
      "iteration 2000 / 20000: loss 1.380484\n",
      "iteration 3000 / 20000: loss 1.515126\n",
      "iteration 4000 / 20000: loss 1.505728\n",
      "iteration 5000 / 20000: loss 1.458183\n",
      "iteration 6000 / 20000: loss 1.456335\n",
      "iteration 7000 / 20000: loss 1.393565\n",
      "iteration 8000 / 20000: loss 1.484920\n",
      "iteration 9000 / 20000: loss 1.542888\n",
      "iteration 10000 / 20000: loss 1.414846\n",
      "iteration 11000 / 20000: loss 1.554146\n",
      "iteration 12000 / 20000: loss 1.458935\n",
      "iteration 13000 / 20000: loss 1.327015\n",
      "iteration 14000 / 20000: loss 1.459090\n",
      "iteration 15000 / 20000: loss 1.418535\n",
      "iteration 16000 / 20000: loss 1.372907\n",
      "iteration 17000 / 20000: loss 1.417583\n",
      "iteration 18000 / 20000: loss 1.354492\n",
      "iteration 19000 / 20000: loss 1.507153\n",
      "Train accuracy:  0.5284756097560975\n",
      "Loss:  1.4327374471997076\n",
      "Test accuracy:  0.487\n",
      " \n",
      "The 8 th training\n",
      "iteration 0 / 20000: loss 2.302599\n",
      "iteration 1000 / 20000: loss 1.735580\n",
      "iteration 2000 / 20000: loss 1.660006\n",
      "iteration 3000 / 20000: loss 1.641840\n",
      "iteration 4000 / 20000: loss 1.552757\n",
      "iteration 5000 / 20000: loss 1.525999\n",
      "iteration 6000 / 20000: loss 1.537742\n",
      "iteration 7000 / 20000: loss 1.520770\n",
      "iteration 8000 / 20000: loss 1.498118\n",
      "iteration 9000 / 20000: loss 1.490166\n",
      "iteration 10000 / 20000: loss 1.433600\n",
      "iteration 11000 / 20000: loss 1.514273\n",
      "iteration 12000 / 20000: loss 1.496435\n",
      "iteration 13000 / 20000: loss 1.476820\n",
      "iteration 14000 / 20000: loss 1.491025\n",
      "iteration 15000 / 20000: loss 1.383511\n",
      "iteration 16000 / 20000: loss 1.353885\n",
      "iteration 17000 / 20000: loss 1.512513\n",
      "iteration 18000 / 20000: loss 1.457897\n",
      "iteration 19000 / 20000: loss 1.469442\n",
      "iteration 0 / 3000: loss 1.514601\n",
      "iteration 0 / 3000: train_acc 0.485000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.572053\n",
      "iteration 1000 / 3000: train_acc 0.425000\n",
      "Reject: 273   Accept: 728\n",
      "iteration 2000 / 3000: loss 1.447138\n",
      "iteration 2000 / 3000: train_acc 0.440000\n",
      "Reject: 1136   Accept: 865\n",
      "iteration 0 / 20000: loss 1.753703\n",
      "iteration 1000 / 20000: loss 1.576112\n",
      "iteration 2000 / 20000: loss 1.579543\n",
      "iteration 3000 / 20000: loss 1.504881\n",
      "iteration 4000 / 20000: loss 1.382841\n",
      "iteration 5000 / 20000: loss 1.273145\n",
      "iteration 6000 / 20000: loss 1.413675\n",
      "iteration 7000 / 20000: loss 1.388623\n",
      "iteration 8000 / 20000: loss 1.373366\n",
      "iteration 9000 / 20000: loss 1.508114\n",
      "iteration 10000 / 20000: loss 1.403266\n",
      "iteration 11000 / 20000: loss 1.495499\n",
      "iteration 12000 / 20000: loss 1.451272\n",
      "iteration 13000 / 20000: loss 1.426995\n",
      "iteration 14000 / 20000: loss 1.469330\n",
      "iteration 15000 / 20000: loss 1.440996\n",
      "iteration 16000 / 20000: loss 1.324594\n",
      "iteration 17000 / 20000: loss 1.527280\n",
      "iteration 18000 / 20000: loss 1.386326\n",
      "iteration 19000 / 20000: loss 1.388565\n",
      "Train accuracy:  0.5179268292682928\n",
      "Loss:  1.4363932808568942\n",
      "Test accuracy:  0.492\n",
      " \n",
      "The 9 th training\n",
      "iteration 0 / 20000: loss 2.302609\n",
      "iteration 1000 / 20000: loss 1.652437\n",
      "iteration 2000 / 20000: loss 1.722685\n",
      "iteration 3000 / 20000: loss 1.663440\n",
      "iteration 4000 / 20000: loss 1.655767\n",
      "iteration 5000 / 20000: loss 1.505345\n",
      "iteration 6000 / 20000: loss 1.697471\n",
      "iteration 7000 / 20000: loss 1.487822\n",
      "iteration 8000 / 20000: loss 1.582474\n",
      "iteration 9000 / 20000: loss 1.512693\n",
      "iteration 10000 / 20000: loss 1.489971\n",
      "iteration 11000 / 20000: loss 1.404592\n",
      "iteration 12000 / 20000: loss 1.457608\n",
      "iteration 13000 / 20000: loss 1.534993\n",
      "iteration 14000 / 20000: loss 1.443077\n",
      "iteration 15000 / 20000: loss 1.562318\n",
      "iteration 16000 / 20000: loss 1.343667\n",
      "iteration 17000 / 20000: loss 1.535189\n",
      "iteration 18000 / 20000: loss 1.412808\n",
      "iteration 19000 / 20000: loss 1.419182\n",
      "iteration 0 / 3000: loss 1.345085\n",
      "iteration 0 / 3000: train_acc 0.555000\n",
      "Reject: 0   Accept: 1\n",
      "iteration 1000 / 3000: loss 1.597403\n",
      "iteration 1000 / 3000: train_acc 0.475000\n",
      "Reject: 323   Accept: 678\n",
      "iteration 2000 / 3000: loss 1.470124\n",
      "iteration 2000 / 3000: train_acc 0.395000\n",
      "Reject: 1243   Accept: 758\n",
      "iteration 0 / 20000: loss 1.788266\n",
      "iteration 1000 / 20000: loss 1.437169\n",
      "iteration 2000 / 20000: loss 1.625031\n",
      "iteration 3000 / 20000: loss 1.368310\n",
      "iteration 4000 / 20000: loss 1.302909\n",
      "iteration 5000 / 20000: loss 1.443343\n",
      "iteration 6000 / 20000: loss 1.458975\n",
      "iteration 7000 / 20000: loss 1.438956\n",
      "iteration 8000 / 20000: loss 1.392293\n",
      "iteration 9000 / 20000: loss 1.316572\n",
      "iteration 10000 / 20000: loss 1.349403\n",
      "iteration 11000 / 20000: loss 1.448129\n",
      "iteration 12000 / 20000: loss 1.482923\n",
      "iteration 13000 / 20000: loss 1.397259\n",
      "iteration 14000 / 20000: loss 1.534648\n",
      "iteration 15000 / 20000: loss 1.319951\n",
      "iteration 16000 / 20000: loss 1.564640\n",
      "iteration 17000 / 20000: loss 1.450073\n",
      "iteration 18000 / 20000: loss 1.322253\n",
      "iteration 19000 / 20000: loss 1.481070\n",
      "Train accuracy:  0.5270731707317073\n",
      "Loss:  1.4263271689932784\n",
      "Test accuracy:  0.487\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Alternative training using SGD and SA \n",
    "\n",
    "#Network Settings\n",
    "\n",
    "input_size = 16 * 16 * 3\n",
    "hidden_size = 30\n",
    "num_classes = 10\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "#np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparams\n",
    "batch_size = 200\n",
    "learning_rate = 5e-4\n",
    "reg = 0.1\n",
    "step_len = 0.001\n",
    "\n",
    "test_acc_list_alter = []\n",
    "train_acc_list_alter = []\n",
    "loss_list_alter = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('The %d th training' % i)\n",
    "    net_bp_alter = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "    stats_bp_alter1 = net_bp_alter.train_bp(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=20000, batch_size=batch_size,\n",
    "                    learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                    reg=reg, print_every = 1000)\n",
    "\n",
    "    stats_sa = net_bp_alter.train_sa(X_train, y_train, X_val, y_val,\n",
    "            num_iters=3000, batch_size=batch_size, step_len = step_len,\n",
    "            reg=reg, T_max = 0.5, T_min = 0.001, print_every = 1000)\n",
    "\n",
    "    stats_bp_alter2 = net_bp_alter.train_bp(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=20000, batch_size=batch_size,\n",
    "                    learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                    reg=reg, print_every = 1000)\n",
    "    \n",
    "    loss_avg = np.mean(stats_bp_alter2['loss_history'][-5000:])\n",
    "    train_acc_avg = np.mean(stats_bp_alter2['train_acc_history'][-5000:])\n",
    "    print('Train accuracy: ', train_acc_avg)\n",
    "    print('Loss: ', loss_avg)\n",
    "    \n",
    "    test_acc = (net_bp_alter.predict(X_test) == y_test).mean()\n",
    "    print('Test accuracy: ', test_acc)\n",
    "    print(' ')\n",
    "    \n",
    "    loss_list_alter.append(loss_avg)\n",
    "    train_acc_list_alter.append(train_acc_avg)\n",
    "    test_acc_list_alter.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCej/AQQ6HT2btXR4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22005589390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.429911367389242\n",
      "0.5306707317073172\n"
     ]
    }
   ],
   "source": [
    "plt.plot(stats_bp_alter2['loss_history'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(stats_bp_alter2['loss_history'][-5000:]))\n",
    "print(np.mean(stats_bp_alter2['train_acc_history'][-5000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative training using advanced SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 th training\n",
      "iteration 0 / 20000: loss 2.302600\n",
      "iteration 1000 / 20000: loss 1.763808\n",
      "iteration 2000 / 20000: loss 1.611288\n",
      "iteration 3000 / 20000: loss 1.659288\n",
      "iteration 4000 / 20000: loss 1.643556\n",
      "iteration 5000 / 20000: loss 1.451654\n",
      "iteration 6000 / 20000: loss 1.470274\n",
      "iteration 7000 / 20000: loss 1.369812\n",
      "iteration 8000 / 20000: loss 1.513991\n",
      "iteration 9000 / 20000: loss 1.528223\n",
      "iteration 10000 / 20000: loss 1.371145\n",
      "iteration 11000 / 20000: loss 1.484118\n",
      "iteration 12000 / 20000: loss 1.423214\n",
      "iteration 13000 / 20000: loss 1.449208\n",
      "iteration 14000 / 20000: loss 1.414100\n",
      "iteration 15000 / 20000: loss 1.468854\n",
      "iteration 16000 / 20000: loss 1.465487\n",
      "iteration 17000 / 20000: loss 1.423159\n",
      "iteration 18000 / 20000: loss 1.410794\n",
      "iteration 19000 / 20000: loss 1.400529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\CBICR_contest\\Source\\head\\neural_net.py:291: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: loss 1.458816\n",
      "iteration 0 / 3000: train_acc 0.485000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "3 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.175884\n",
      "iteration 1000 / 3000: train_acc 0.340000\n",
      "Ratio: 0.10009343312798379   Reject: 324   Accept: 677\n",
      "23040 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.029130\n",
      "iteration 2000 / 3000: train_acc 0.320000\n",
      "Ratio: 1.0233304046496126e-12   Reject: 1274   Accept: 727\n",
      "9521 / 23040 W1 params change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\CBICR_contest\\Source\\head\\neural_net.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "  correct_logprobs = -np.log(probs[np.arange(N),y])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 20000: loss 2.143639\n",
      "iteration 1000 / 20000: loss 1.552555\n",
      "iteration 2000 / 20000: loss 1.672996\n",
      "iteration 3000 / 20000: loss 1.650748\n",
      "iteration 4000 / 20000: loss 1.684733\n",
      "iteration 5000 / 20000: loss 1.472898\n",
      "iteration 6000 / 20000: loss 1.437933\n",
      "iteration 7000 / 20000: loss 1.575058\n",
      "iteration 8000 / 20000: loss 1.465168\n",
      "iteration 9000 / 20000: loss 1.445878\n",
      "iteration 10000 / 20000: loss 1.522634\n",
      "iteration 11000 / 20000: loss 1.477105\n",
      "iteration 12000 / 20000: loss 1.455301\n",
      "iteration 13000 / 20000: loss 1.419560\n",
      "iteration 14000 / 20000: loss 1.580494\n",
      "iteration 15000 / 20000: loss 1.525101\n",
      "iteration 16000 / 20000: loss 1.395731\n",
      "iteration 17000 / 20000: loss 1.520084\n",
      "iteration 18000 / 20000: loss 1.428494\n",
      "iteration 19000 / 20000: loss 1.542081\n",
      "Train accuracy:  0.5213414634146343\n",
      "Loss:  1.4821758546618127\n",
      "Test accuracy:  0.495\n",
      " \n",
      "The 1 th training\n",
      "iteration 0 / 20000: loss 2.302617\n",
      "iteration 1000 / 20000: loss 1.860653\n",
      "iteration 2000 / 20000: loss 1.602671\n",
      "iteration 3000 / 20000: loss 1.531247\n",
      "iteration 4000 / 20000: loss 1.523227\n",
      "iteration 5000 / 20000: loss 1.484697\n",
      "iteration 6000 / 20000: loss 1.487068\n",
      "iteration 7000 / 20000: loss 1.455661\n",
      "iteration 8000 / 20000: loss 1.596073\n",
      "iteration 9000 / 20000: loss 1.587458\n",
      "iteration 10000 / 20000: loss 1.540675\n",
      "iteration 11000 / 20000: loss 1.461147\n",
      "iteration 12000 / 20000: loss 1.454377\n",
      "iteration 13000 / 20000: loss 1.424361\n",
      "iteration 14000 / 20000: loss 1.463063\n",
      "iteration 15000 / 20000: loss 1.503259\n",
      "iteration 16000 / 20000: loss 1.307693\n",
      "iteration 17000 / 20000: loss 1.407297\n",
      "iteration 18000 / 20000: loss 1.398844\n",
      "iteration 19000 / 20000: loss 1.562381\n",
      "iteration 0 / 3000: loss 1.500727\n",
      "iteration 0 / 3000: train_acc 0.535000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "19232 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.264993\n",
      "iteration 1000 / 3000: train_acc 0.355000\n",
      "Ratio: 0.549909304391302   Reject: 345   Accept: 656\n",
      "291 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 1.948846\n",
      "iteration 2000 / 3000: train_acc 0.305000\n",
      "Ratio: 6.003136615090812e-25   Reject: 1278   Accept: 723\n",
      "2 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.291471\n",
      "iteration 1000 / 20000: loss 1.770730\n",
      "iteration 2000 / 20000: loss 1.580222\n",
      "iteration 3000 / 20000: loss 1.507022\n",
      "iteration 4000 / 20000: loss 1.482365\n",
      "iteration 5000 / 20000: loss 1.504367\n",
      "iteration 6000 / 20000: loss 1.434085\n",
      "iteration 7000 / 20000: loss 1.373574\n",
      "iteration 8000 / 20000: loss 1.471924\n",
      "iteration 9000 / 20000: loss 1.471584\n",
      "iteration 10000 / 20000: loss 1.482791\n",
      "iteration 11000 / 20000: loss 1.454956\n",
      "iteration 12000 / 20000: loss 1.397261\n",
      "iteration 13000 / 20000: loss 1.486719\n",
      "iteration 14000 / 20000: loss 1.358289\n",
      "iteration 15000 / 20000: loss 1.390745\n",
      "iteration 16000 / 20000: loss 1.466107\n",
      "iteration 17000 / 20000: loss 1.519132\n",
      "iteration 18000 / 20000: loss 1.379545\n",
      "iteration 19000 / 20000: loss 1.616320\n",
      "Train accuracy:  0.5159756097560975\n",
      "Loss:  1.4759278480951235\n",
      "Test accuracy:  0.484\n",
      " \n",
      "The 2 th training\n",
      "iteration 0 / 20000: loss 2.302615\n",
      "iteration 1000 / 20000: loss 1.759028\n",
      "iteration 2000 / 20000: loss 1.550858\n",
      "iteration 3000 / 20000: loss 1.580111\n",
      "iteration 4000 / 20000: loss 1.561246\n",
      "iteration 5000 / 20000: loss 1.500136\n",
      "iteration 6000 / 20000: loss 1.425734\n",
      "iteration 7000 / 20000: loss 1.419732\n",
      "iteration 8000 / 20000: loss 1.496762\n",
      "iteration 9000 / 20000: loss 1.411758\n",
      "iteration 10000 / 20000: loss 1.512259\n",
      "iteration 11000 / 20000: loss 1.569545\n",
      "iteration 12000 / 20000: loss 1.502313\n",
      "iteration 13000 / 20000: loss 1.574203\n",
      "iteration 14000 / 20000: loss 1.539242\n",
      "iteration 15000 / 20000: loss 1.486052\n",
      "iteration 16000 / 20000: loss 1.376879\n",
      "iteration 17000 / 20000: loss 1.453410\n",
      "iteration 18000 / 20000: loss 1.405435\n",
      "iteration 19000 / 20000: loss 1.486050\n",
      "iteration 0 / 3000: loss 1.492991\n",
      "iteration 0 / 3000: train_acc 0.485000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "8 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.348317\n",
      "iteration 1000 / 3000: train_acc 0.300000\n",
      "Ratio: 0.10890733744713454   Reject: 335   Accept: 666\n",
      "2013 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.231980\n",
      "iteration 2000 / 3000: train_acc 0.300000\n",
      "Ratio: 8.491993660924928e-24   Reject: 1279   Accept: 722\n",
      "6 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.731940\n",
      "iteration 1000 / 20000: loss 1.783292\n",
      "iteration 2000 / 20000: loss 1.636226\n",
      "iteration 3000 / 20000: loss 1.623883\n",
      "iteration 4000 / 20000: loss 1.427600\n",
      "iteration 5000 / 20000: loss 1.622637\n",
      "iteration 6000 / 20000: loss 1.571131\n",
      "iteration 7000 / 20000: loss 1.402170\n",
      "iteration 8000 / 20000: loss 1.671191\n",
      "iteration 9000 / 20000: loss 1.552651\n",
      "iteration 10000 / 20000: loss 1.591839\n",
      "iteration 11000 / 20000: loss 1.451252\n",
      "iteration 12000 / 20000: loss 1.432052\n",
      "iteration 13000 / 20000: loss 1.487508\n",
      "iteration 14000 / 20000: loss 1.545958\n",
      "iteration 15000 / 20000: loss 1.502015\n",
      "iteration 16000 / 20000: loss 1.366393\n",
      "iteration 17000 / 20000: loss 1.533503\n",
      "iteration 18000 / 20000: loss 1.487289\n",
      "iteration 19000 / 20000: loss 1.298700\n",
      "Train accuracy:  0.5167073170731707\n",
      "Loss:  1.4898566432857776\n",
      "Test accuracy:  0.491\n",
      " \n",
      "The 3 th training\n",
      "iteration 0 / 20000: loss 2.302621\n",
      "iteration 1000 / 20000: loss 1.763206\n",
      "iteration 2000 / 20000: loss 1.582361\n",
      "iteration 3000 / 20000: loss 1.571176\n",
      "iteration 4000 / 20000: loss 1.467846\n",
      "iteration 5000 / 20000: loss 1.559818\n",
      "iteration 6000 / 20000: loss 1.588890\n",
      "iteration 7000 / 20000: loss 1.560450\n",
      "iteration 8000 / 20000: loss 1.557395\n",
      "iteration 9000 / 20000: loss 1.503520\n",
      "iteration 10000 / 20000: loss 1.440381\n",
      "iteration 11000 / 20000: loss 1.438047\n",
      "iteration 12000 / 20000: loss 1.537327\n",
      "iteration 13000 / 20000: loss 1.457287\n",
      "iteration 14000 / 20000: loss 1.396630\n",
      "iteration 15000 / 20000: loss 1.513616\n",
      "iteration 16000 / 20000: loss 1.387028\n",
      "iteration 17000 / 20000: loss 1.457502\n",
      "iteration 18000 / 20000: loss 1.450676\n",
      "iteration 19000 / 20000: loss 1.471988\n",
      "iteration 0 / 3000: loss 1.530204\n",
      "iteration 0 / 3000: train_acc 0.465000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "158 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.251307\n",
      "iteration 1000 / 3000: train_acc 0.280000\n",
      "Ratio: 4.9645544249611134e-05   Reject: 379   Accept: 622\n",
      "28 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.251307\n",
      "iteration 2000 / 3000: train_acc 0.310000\n",
      "Ratio: 3.8585833297618614e-23   Reject: 1379   Accept: 622\n",
      "21799 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.957502\n",
      "iteration 1000 / 20000: loss 1.636295\n",
      "iteration 2000 / 20000: loss 1.648890\n",
      "iteration 3000 / 20000: loss 1.483871\n",
      "iteration 4000 / 20000: loss 1.563380\n",
      "iteration 5000 / 20000: loss 1.571190\n",
      "iteration 6000 / 20000: loss 1.525721\n",
      "iteration 7000 / 20000: loss 1.563685\n",
      "iteration 8000 / 20000: loss 1.563597\n",
      "iteration 9000 / 20000: loss 1.590985\n",
      "iteration 10000 / 20000: loss 1.559459\n",
      "iteration 11000 / 20000: loss 1.543023\n",
      "iteration 12000 / 20000: loss 1.435062\n",
      "iteration 13000 / 20000: loss 1.492051\n",
      "iteration 14000 / 20000: loss 1.495759\n",
      "iteration 15000 / 20000: loss 1.614743\n",
      "iteration 16000 / 20000: loss 1.447026\n",
      "iteration 17000 / 20000: loss 1.491235\n",
      "iteration 18000 / 20000: loss 1.459891\n",
      "iteration 19000 / 20000: loss 1.430535\n",
      "Train accuracy:  0.5196951219512196\n",
      "Loss:  1.4801322411914477\n",
      "Test accuracy:  0.486\n",
      " \n",
      "The 4 th training\n",
      "iteration 0 / 20000: loss 2.302618\n",
      "iteration 1000 / 20000: loss 1.628131\n",
      "iteration 2000 / 20000: loss 1.664635\n",
      "iteration 3000 / 20000: loss 1.654194\n",
      "iteration 4000 / 20000: loss 1.468419\n",
      "iteration 5000 / 20000: loss 1.508553\n",
      "iteration 6000 / 20000: loss 1.650509\n",
      "iteration 7000 / 20000: loss 1.575798\n",
      "iteration 8000 / 20000: loss 1.422482\n",
      "iteration 9000 / 20000: loss 1.627929\n",
      "iteration 10000 / 20000: loss 1.583282\n",
      "iteration 11000 / 20000: loss 1.540660\n",
      "iteration 12000 / 20000: loss 1.509883\n",
      "iteration 13000 / 20000: loss 1.559586\n",
      "iteration 14000 / 20000: loss 1.394104\n",
      "iteration 15000 / 20000: loss 1.501313\n",
      "iteration 16000 / 20000: loss 1.522028\n",
      "iteration 17000 / 20000: loss 1.369640\n",
      "iteration 18000 / 20000: loss 1.456507\n",
      "iteration 19000 / 20000: loss 1.472407\n",
      "iteration 0 / 3000: loss 1.327628\n",
      "iteration 0 / 3000: train_acc 0.555000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "141 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.446265\n",
      "iteration 1000 / 3000: train_acc 0.215000\n",
      "Ratio: 0.0029104024822061794   Reject: 386   Accept: 615\n",
      "23 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.408300\n",
      "iteration 2000 / 3000: train_acc 0.320000\n",
      "Ratio: 1.322711786586465e-17   Reject: 1364   Accept: 637\n",
      "5512 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.933017\n",
      "iteration 1000 / 20000: loss 1.635798\n",
      "iteration 2000 / 20000: loss 1.552313\n",
      "iteration 3000 / 20000: loss 1.591589\n",
      "iteration 4000 / 20000: loss 1.432991\n",
      "iteration 5000 / 20000: loss 1.546319\n",
      "iteration 6000 / 20000: loss 1.595562\n",
      "iteration 7000 / 20000: loss 1.482558\n",
      "iteration 8000 / 20000: loss 1.369779\n",
      "iteration 9000 / 20000: loss 1.465600\n",
      "iteration 10000 / 20000: loss 1.543370\n",
      "iteration 11000 / 20000: loss 1.504240\n",
      "iteration 12000 / 20000: loss 1.490612\n",
      "iteration 13000 / 20000: loss 1.457815\n",
      "iteration 14000 / 20000: loss 1.632260\n",
      "iteration 15000 / 20000: loss 1.518420\n",
      "iteration 16000 / 20000: loss 1.510668\n",
      "iteration 17000 / 20000: loss 1.492011\n",
      "iteration 18000 / 20000: loss 1.585033\n",
      "iteration 19000 / 20000: loss 1.488734\n",
      "Train accuracy:  0.5182317073170731\n",
      "Loss:  1.4869556591011963\n",
      "Test accuracy:  0.487\n",
      " \n",
      "The 5 th training\n",
      "iteration 0 / 20000: loss 2.302597\n",
      "iteration 1000 / 20000: loss 1.722555\n",
      "iteration 2000 / 20000: loss 1.691143\n",
      "iteration 3000 / 20000: loss 1.560633\n",
      "iteration 4000 / 20000: loss 1.564780\n",
      "iteration 5000 / 20000: loss 1.510751\n",
      "iteration 6000 / 20000: loss 1.474527\n",
      "iteration 7000 / 20000: loss 1.492625\n",
      "iteration 8000 / 20000: loss 1.584403\n",
      "iteration 9000 / 20000: loss 1.616459\n",
      "iteration 10000 / 20000: loss 1.443150\n",
      "iteration 11000 / 20000: loss 1.586667\n",
      "iteration 12000 / 20000: loss 1.425703\n",
      "iteration 13000 / 20000: loss 1.457929\n",
      "iteration 14000 / 20000: loss 1.485655\n",
      "iteration 15000 / 20000: loss 1.470332\n",
      "iteration 16000 / 20000: loss 1.472986\n",
      "iteration 17000 / 20000: loss 1.424218\n",
      "iteration 18000 / 20000: loss 1.411831\n",
      "iteration 19000 / 20000: loss 1.442795\n",
      "iteration 0 / 3000: loss 1.577868\n",
      "iteration 0 / 3000: train_acc 0.465000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "9 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.562517\n",
      "iteration 1000 / 3000: train_acc 0.290000\n",
      "Ratio: 18.464407879730118   Reject: 358   Accept: 643\n",
      "7 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.231079\n",
      "iteration 2000 / 3000: train_acc 0.300000\n",
      "Ratio: 2.030335652930485e-22   Reject: 1308   Accept: 693\n",
      "8845 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.634486\n",
      "iteration 1000 / 20000: loss 1.767693\n",
      "iteration 2000 / 20000: loss 1.683997\n",
      "iteration 3000 / 20000: loss 1.649328\n",
      "iteration 4000 / 20000: loss 1.531479\n",
      "iteration 5000 / 20000: loss 1.559840\n",
      "iteration 6000 / 20000: loss 1.455500\n",
      "iteration 7000 / 20000: loss 1.531343\n",
      "iteration 8000 / 20000: loss 1.653736\n",
      "iteration 9000 / 20000: loss 1.480581\n",
      "iteration 10000 / 20000: loss 1.596908\n",
      "iteration 11000 / 20000: loss 1.641199\n",
      "iteration 12000 / 20000: loss 1.454472\n",
      "iteration 13000 / 20000: loss 1.380840\n",
      "iteration 14000 / 20000: loss 1.510813\n",
      "iteration 15000 / 20000: loss 1.485261\n",
      "iteration 16000 / 20000: loss 1.521405\n",
      "iteration 17000 / 20000: loss 1.427187\n",
      "iteration 18000 / 20000: loss 1.379865\n",
      "iteration 19000 / 20000: loss 1.641244\n",
      "Train accuracy:  0.5147560975609755\n",
      "Loss:  1.50562799098162\n",
      "Test accuracy:  0.478\n",
      " \n",
      "The 6 th training\n",
      "iteration 0 / 20000: loss 2.302603\n",
      "iteration 1000 / 20000: loss 1.799644\n",
      "iteration 2000 / 20000: loss 1.504415\n",
      "iteration 3000 / 20000: loss 1.507289\n",
      "iteration 4000 / 20000: loss 1.551442\n",
      "iteration 5000 / 20000: loss 1.387366\n",
      "iteration 6000 / 20000: loss 1.536767\n",
      "iteration 7000 / 20000: loss 1.484735\n",
      "iteration 8000 / 20000: loss 1.440048\n",
      "iteration 9000 / 20000: loss 1.474584\n",
      "iteration 10000 / 20000: loss 1.497650\n",
      "iteration 11000 / 20000: loss 1.475779\n",
      "iteration 12000 / 20000: loss 1.550064\n",
      "iteration 13000 / 20000: loss 1.465347\n",
      "iteration 14000 / 20000: loss 1.441975\n",
      "iteration 15000 / 20000: loss 1.303782\n",
      "iteration 16000 / 20000: loss 1.380032\n",
      "iteration 17000 / 20000: loss 1.524879\n",
      "iteration 18000 / 20000: loss 1.358195\n",
      "iteration 19000 / 20000: loss 1.448031\n",
      "iteration 0 / 3000: loss 1.466195\n",
      "iteration 0 / 3000: train_acc 0.515000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "11480 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.076546\n",
      "iteration 1000 / 3000: train_acc 0.340000\n",
      "Ratio: 0.03259970590467263   Reject: 362   Accept: 639\n",
      "1369 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.071895\n",
      "iteration 2000 / 3000: train_acc 0.380000\n",
      "Ratio: 4.184196752190733e-18   Reject: 1316   Accept: 685\n",
      "18 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.537100\n",
      "iteration 1000 / 20000: loss 1.663140\n",
      "iteration 2000 / 20000: loss 1.459350\n",
      "iteration 3000 / 20000: loss 1.592575\n",
      "iteration 4000 / 20000: loss 1.516787\n",
      "iteration 5000 / 20000: loss 1.431554\n",
      "iteration 6000 / 20000: loss 1.425859\n",
      "iteration 7000 / 20000: loss 1.522299\n",
      "iteration 8000 / 20000: loss 1.522590\n",
      "iteration 9000 / 20000: loss 1.507358\n",
      "iteration 10000 / 20000: loss 1.532873\n",
      "iteration 11000 / 20000: loss 1.503011\n",
      "iteration 12000 / 20000: loss 1.565231\n",
      "iteration 13000 / 20000: loss 1.573648\n",
      "iteration 14000 / 20000: loss 1.606394\n",
      "iteration 15000 / 20000: loss 1.619367\n",
      "iteration 16000 / 20000: loss 1.598261\n",
      "iteration 17000 / 20000: loss 1.527643\n",
      "iteration 18000 / 20000: loss 1.523522\n",
      "iteration 19000 / 20000: loss 1.549549\n",
      "Train accuracy:  0.5159756097560975\n",
      "Loss:  1.4880916359855576\n",
      "Test accuracy:  0.488\n",
      " \n",
      "The 7 th training\n",
      "iteration 0 / 20000: loss 2.302615\n",
      "iteration 1000 / 20000: loss 1.759223\n",
      "iteration 2000 / 20000: loss 1.665905\n",
      "iteration 3000 / 20000: loss 1.581306\n",
      "iteration 4000 / 20000: loss 1.491119\n",
      "iteration 5000 / 20000: loss 1.500537\n",
      "iteration 6000 / 20000: loss 1.450834\n",
      "iteration 7000 / 20000: loss 1.464234\n",
      "iteration 8000 / 20000: loss 1.530062\n",
      "iteration 9000 / 20000: loss 1.496438\n",
      "iteration 10000 / 20000: loss 1.378832\n",
      "iteration 11000 / 20000: loss 1.357877\n",
      "iteration 12000 / 20000: loss 1.473105\n",
      "iteration 13000 / 20000: loss 1.406863\n",
      "iteration 14000 / 20000: loss 1.514867\n",
      "iteration 15000 / 20000: loss 1.560432\n",
      "iteration 16000 / 20000: loss 1.429052\n",
      "iteration 17000 / 20000: loss 1.462167\n",
      "iteration 18000 / 20000: loss 1.496815\n",
      "iteration 19000 / 20000: loss 1.404832\n",
      "iteration 0 / 3000: loss 1.629222\n",
      "iteration 0 / 3000: train_acc 0.455000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "105 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.598485\n",
      "iteration 1000 / 3000: train_acc 0.265000\n",
      "Ratio: 0.26519026872525203   Reject: 298   Accept: 703\n",
      "33 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.346311\n",
      "iteration 2000 / 3000: train_acc 0.305000\n",
      "Ratio: 2.3172391463883228e-20   Reject: 1264   Accept: 737\n",
      "784 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.908613\n",
      "iteration 1000 / 20000: loss 1.703318\n",
      "iteration 2000 / 20000: loss 1.574515\n",
      "iteration 3000 / 20000: loss 1.558595\n",
      "iteration 4000 / 20000: loss 1.662526\n",
      "iteration 5000 / 20000: loss 1.491335\n",
      "iteration 6000 / 20000: loss 1.500625\n",
      "iteration 7000 / 20000: loss 1.483595\n",
      "iteration 8000 / 20000: loss 1.497124\n",
      "iteration 9000 / 20000: loss 1.531249\n",
      "iteration 10000 / 20000: loss 1.516998\n",
      "iteration 11000 / 20000: loss 1.576586\n",
      "iteration 12000 / 20000: loss 1.536467\n",
      "iteration 13000 / 20000: loss 1.438122\n",
      "iteration 14000 / 20000: loss 1.491584\n",
      "iteration 15000 / 20000: loss 1.559405\n",
      "iteration 16000 / 20000: loss 1.445138\n",
      "iteration 17000 / 20000: loss 1.499470\n",
      "iteration 18000 / 20000: loss 1.442092\n",
      "iteration 19000 / 20000: loss 1.470646\n",
      "Train accuracy:  0.5131097560975608\n",
      "Loss:  1.5063750365178608\n",
      "Test accuracy:  0.471\n",
      " \n",
      "The 8 th training\n",
      "iteration 0 / 20000: loss 2.302615\n",
      "iteration 1000 / 20000: loss 1.688069\n",
      "iteration 2000 / 20000: loss 1.566082\n",
      "iteration 3000 / 20000: loss 1.527311\n",
      "iteration 4000 / 20000: loss 1.520004\n",
      "iteration 5000 / 20000: loss 1.656927\n",
      "iteration 6000 / 20000: loss 1.463073\n",
      "iteration 7000 / 20000: loss 1.450920\n",
      "iteration 8000 / 20000: loss 1.534895\n",
      "iteration 9000 / 20000: loss 1.436640\n",
      "iteration 10000 / 20000: loss 1.582201\n",
      "iteration 11000 / 20000: loss 1.644883\n",
      "iteration 12000 / 20000: loss 1.433728\n",
      "iteration 13000 / 20000: loss 1.415617\n",
      "iteration 14000 / 20000: loss 1.444407\n",
      "iteration 15000 / 20000: loss 1.425117\n",
      "iteration 16000 / 20000: loss 1.474398\n",
      "iteration 17000 / 20000: loss 1.451623\n",
      "iteration 18000 / 20000: loss 1.538956\n",
      "iteration 19000 / 20000: loss 1.354584\n",
      "iteration 0 / 3000: loss 1.475791\n",
      "iteration 0 / 3000: train_acc 0.510000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "10423 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 1.879480\n",
      "iteration 1000 / 3000: train_acc 0.430000\n",
      "Ratio: 2.9225740242529072   Reject: 300   Accept: 701\n",
      "20171 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 1.695171\n",
      "iteration 2000 / 3000: train_acc 0.320000\n",
      "Ratio: 8.917080656428129e-21   Reject: 1254   Accept: 747\n",
      "236 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.046065\n",
      "iteration 1000 / 20000: loss 1.598006\n",
      "iteration 2000 / 20000: loss 1.587803\n",
      "iteration 3000 / 20000: loss 1.443433\n",
      "iteration 4000 / 20000: loss 1.626111\n",
      "iteration 5000 / 20000: loss 1.509704\n",
      "iteration 6000 / 20000: loss 1.505535\n",
      "iteration 7000 / 20000: loss 1.464942\n",
      "iteration 8000 / 20000: loss 1.532526\n",
      "iteration 9000 / 20000: loss 1.664361\n",
      "iteration 10000 / 20000: loss 1.373319\n",
      "iteration 11000 / 20000: loss 1.428347\n",
      "iteration 12000 / 20000: loss 1.448085\n",
      "iteration 13000 / 20000: loss 1.362667\n",
      "iteration 14000 / 20000: loss 1.370083\n",
      "iteration 15000 / 20000: loss 1.379724\n",
      "iteration 16000 / 20000: loss 1.522830\n",
      "iteration 17000 / 20000: loss 1.413987\n",
      "iteration 18000 / 20000: loss 1.289204\n",
      "iteration 19000 / 20000: loss 1.404553\n",
      "Train accuracy:  0.519390243902439\n",
      "Loss:  1.459126830302326\n",
      "Test accuracy:  0.493\n",
      " \n",
      "The 9 th training\n",
      "iteration 0 / 20000: loss 2.302600\n",
      "iteration 1000 / 20000: loss 1.799607\n",
      "iteration 2000 / 20000: loss 1.622130\n",
      "iteration 3000 / 20000: loss 1.588538\n",
      "iteration 4000 / 20000: loss 1.533665\n",
      "iteration 5000 / 20000: loss 1.595396\n",
      "iteration 6000 / 20000: loss 1.464676\n",
      "iteration 7000 / 20000: loss 1.674199\n",
      "iteration 8000 / 20000: loss 1.423798\n",
      "iteration 9000 / 20000: loss 1.379222\n",
      "iteration 10000 / 20000: loss 1.507889\n",
      "iteration 11000 / 20000: loss 1.513786\n",
      "iteration 12000 / 20000: loss 1.331148\n",
      "iteration 13000 / 20000: loss 1.554214\n",
      "iteration 14000 / 20000: loss 1.486126\n",
      "iteration 15000 / 20000: loss 1.468951\n",
      "iteration 16000 / 20000: loss 1.416990\n",
      "iteration 17000 / 20000: loss 1.469962\n",
      "iteration 18000 / 20000: loss 1.470265\n",
      "iteration 19000 / 20000: loss 1.457382\n",
      "iteration 0 / 3000: loss 1.478954\n",
      "iteration 0 / 3000: train_acc 0.515000\n",
      "Ratio: inf   Reject: 0   Accept: 1\n",
      "544 / 23040 W1 params change\n",
      "iteration 1000 / 3000: loss 2.185026\n",
      "iteration 1000 / 3000: train_acc 0.290000\n",
      "Ratio: 1.997609625529356   Reject: 359   Accept: 642\n",
      "1 / 23040 W1 params change\n",
      "iteration 2000 / 3000: loss 2.050949\n",
      "iteration 2000 / 3000: train_acc 0.255000\n",
      "Ratio: 3.652277175749587e-19   Reject: 1311   Accept: 690\n",
      "28 / 23040 W1 params change\n",
      "iteration 0 / 20000: loss 2.449194\n",
      "iteration 1000 / 20000: loss 1.762222\n",
      "iteration 2000 / 20000: loss 1.490796\n",
      "iteration 3000 / 20000: loss 1.567631\n",
      "iteration 4000 / 20000: loss 1.488334\n",
      "iteration 5000 / 20000: loss 1.518486\n",
      "iteration 6000 / 20000: loss 1.418556\n",
      "iteration 7000 / 20000: loss 1.547615\n",
      "iteration 8000 / 20000: loss 1.405905\n",
      "iteration 9000 / 20000: loss 1.494508\n",
      "iteration 10000 / 20000: loss 1.655503\n",
      "iteration 11000 / 20000: loss 1.553265\n",
      "iteration 12000 / 20000: loss 1.494805\n",
      "iteration 13000 / 20000: loss 1.545738\n",
      "iteration 14000 / 20000: loss 1.528938\n",
      "iteration 15000 / 20000: loss 1.409052\n",
      "iteration 16000 / 20000: loss 1.453013\n",
      "iteration 17000 / 20000: loss 1.442393\n",
      "iteration 18000 / 20000: loss 1.532386\n",
      "iteration 19000 / 20000: loss 1.448267\n",
      "Train accuracy:  0.5186585365853659\n",
      "Loss:  1.476674186991771\n",
      "Test accuracy:  0.49\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Alternative training using SGD and SA \n",
    "\n",
    "#Network Settings\n",
    "\n",
    "input_size = 16 * 16 * 3\n",
    "hidden_size = 30\n",
    "num_classes = 10\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "#np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparams\n",
    "batch_size = 200\n",
    "learning_rate = 5e-4\n",
    "reg = 0.1\n",
    "step_len = 0.001\n",
    "\n",
    "punish_strength = 1\n",
    "if_sparse = True\n",
    "\n",
    "test_acc_list_alter_adv = []\n",
    "train_acc_list_alter_adv = []\n",
    "loss_list_alter_adv = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('The %d th training' % i)\n",
    "    net_bp_adv_alter = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "    stats_bp_adv_alter1 = net_bp_adv_alter.train_bp(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=20000, batch_size=batch_size,\n",
    "                    learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                    reg=reg, verbose=True, print_every = 1000)\n",
    "\n",
    "    stats_sa_adv = net_bp_adv_alter.train_sa_sparse(X_train, y_train, X_val, y_val,\n",
    "            num_iters=3000, batch_size=batch_size, step_len = step_len,\n",
    "            reg=reg, T_max = 0.5, T_min = 0.001, \n",
    "            if_sparse = if_sparse, punish_strength = punish_strength, \n",
    "            verbose=True, print_every = 1000)\n",
    "\n",
    "    stats_bp_adv_alter2 = net_bp_adv_alter.train_bp(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=20000, batch_size=batch_size,\n",
    "                    learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                    reg=reg, verbose=True, print_every = 1000)\n",
    "\n",
    "    loss_avg = np.mean(stats_bp_adv_alter2['loss_history'][-5000:])\n",
    "    train_acc_avg = np.mean(stats_bp_adv_alter2['train_acc_history'][-5000:])\n",
    "    print('Train accuracy: ', train_acc_avg)\n",
    "    print('Loss: ', loss_avg)\n",
    "    \n",
    "    test_acc = (net_bp_adv_alter.predict(X_test) == y_test).mean()\n",
    "    print('Test accuracy: ', test_acc)\n",
    "    print(' ')\n",
    "    \n",
    "    loss_list_alter_adv.append(loss_avg)\n",
    "    train_acc_list_alter_adv.append(train_acc_avg)\n",
    "    test_acc_list_alter_adv.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f02569550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.525279270473765\n",
      "0.5055487804878048\n"
     ]
    }
   ],
   "source": [
    "plt.plot(stats_bp_adv_alter2['loss_history'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(stats_bp_adv_alter2['loss_history'][-5000:]))\n",
    "print(np.mean(stats_bp_adv_alter2['train_acc_history'][-5000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training only using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 th training\n",
      "iteration 0 / 40000: loss 2.302603\n",
      "iteration 1000 / 40000: loss 1.709209\n",
      "iteration 2000 / 40000: loss 1.684392\n",
      "iteration 3000 / 40000: loss 1.566843\n",
      "iteration 4000 / 40000: loss 1.568243\n",
      "iteration 5000 / 40000: loss 1.682842\n",
      "iteration 6000 / 40000: loss 1.476758\n",
      "iteration 7000 / 40000: loss 1.493141\n",
      "iteration 8000 / 40000: loss 1.473971\n",
      "iteration 9000 / 40000: loss 1.656435\n",
      "iteration 10000 / 40000: loss 1.567824\n",
      "iteration 11000 / 40000: loss 1.657160\n",
      "iteration 12000 / 40000: loss 1.441435\n",
      "iteration 13000 / 40000: loss 1.500059\n",
      "iteration 14000 / 40000: loss 1.413653\n",
      "iteration 15000 / 40000: loss 1.427535\n",
      "iteration 16000 / 40000: loss 1.449510\n",
      "iteration 17000 / 40000: loss 1.464323\n",
      "iteration 18000 / 40000: loss 1.446418\n",
      "iteration 19000 / 40000: loss 1.514611\n",
      "iteration 20000 / 40000: loss 1.385371\n",
      "iteration 21000 / 40000: loss 1.440411\n",
      "iteration 22000 / 40000: loss 1.475636\n",
      "iteration 23000 / 40000: loss 1.512339\n",
      "iteration 24000 / 40000: loss 1.388340\n",
      "iteration 25000 / 40000: loss 1.578508\n",
      "iteration 26000 / 40000: loss 1.509532\n",
      "iteration 27000 / 40000: loss 1.457078\n",
      "iteration 28000 / 40000: loss 1.367158\n",
      "iteration 29000 / 40000: loss 1.412353\n",
      "iteration 30000 / 40000: loss 1.430006\n",
      "iteration 31000 / 40000: loss 1.430893\n",
      "iteration 32000 / 40000: loss 1.523595\n",
      "iteration 33000 / 40000: loss 1.420051\n",
      "iteration 34000 / 40000: loss 1.493255\n",
      "iteration 35000 / 40000: loss 1.519258\n",
      "iteration 36000 / 40000: loss 1.466753\n",
      "iteration 37000 / 40000: loss 1.465004\n",
      "iteration 38000 / 40000: loss 1.496242\n",
      "iteration 39000 / 40000: loss 1.380073\n",
      "Train accuracy:  0.4918292682926829\n",
      "Loss:  1.4643239713007787\n",
      "Test accuracy:  0.48\n",
      " \n",
      "The 1 th training\n",
      "iteration 0 / 40000: loss 2.302612\n",
      "iteration 1000 / 40000: loss 1.757448\n",
      "iteration 2000 / 40000: loss 1.534985\n",
      "iteration 3000 / 40000: loss 1.591124\n",
      "iteration 4000 / 40000: loss 1.459249\n",
      "iteration 5000 / 40000: loss 1.563594\n",
      "iteration 6000 / 40000: loss 1.453990\n",
      "iteration 7000 / 40000: loss 1.579728\n",
      "iteration 8000 / 40000: loss 1.651122\n",
      "iteration 9000 / 40000: loss 1.551858\n",
      "iteration 10000 / 40000: loss 1.483980\n",
      "iteration 11000 / 40000: loss 1.532442\n",
      "iteration 12000 / 40000: loss 1.437820\n",
      "iteration 13000 / 40000: loss 1.582049\n",
      "iteration 14000 / 40000: loss 1.466867\n",
      "iteration 15000 / 40000: loss 1.381282\n",
      "iteration 16000 / 40000: loss 1.427941\n",
      "iteration 17000 / 40000: loss 1.482831\n",
      "iteration 18000 / 40000: loss 1.418488\n",
      "iteration 19000 / 40000: loss 1.527598\n",
      "iteration 20000 / 40000: loss 1.425512\n",
      "iteration 21000 / 40000: loss 1.437643\n",
      "iteration 22000 / 40000: loss 1.478517\n",
      "iteration 23000 / 40000: loss 1.459564\n",
      "iteration 24000 / 40000: loss 1.508122\n",
      "iteration 25000 / 40000: loss 1.546907\n",
      "iteration 26000 / 40000: loss 1.561998\n",
      "iteration 27000 / 40000: loss 1.471383\n",
      "iteration 28000 / 40000: loss 1.444933\n",
      "iteration 29000 / 40000: loss 1.491092\n",
      "iteration 30000 / 40000: loss 1.544755\n",
      "iteration 31000 / 40000: loss 1.428365\n",
      "iteration 32000 / 40000: loss 1.484937\n",
      "iteration 33000 / 40000: loss 1.439951\n",
      "iteration 34000 / 40000: loss 1.330066\n",
      "iteration 35000 / 40000: loss 1.486091\n",
      "iteration 36000 / 40000: loss 1.440493\n",
      "iteration 37000 / 40000: loss 1.406645\n",
      "iteration 38000 / 40000: loss 1.592004\n",
      "iteration 39000 / 40000: loss 1.434350\n",
      "Train accuracy:  0.49189024390243896\n",
      "Loss:  1.464092760081088\n",
      "Test accuracy:  0.467\n",
      " \n",
      "The 2 th training\n",
      "iteration 0 / 40000: loss 2.302612\n",
      "iteration 1000 / 40000: loss 1.747328\n",
      "iteration 2000 / 40000: loss 1.591273\n",
      "iteration 3000 / 40000: loss 1.496494\n",
      "iteration 4000 / 40000: loss 1.525897\n",
      "iteration 5000 / 40000: loss 1.423400\n",
      "iteration 6000 / 40000: loss 1.432690\n",
      "iteration 7000 / 40000: loss 1.490003\n",
      "iteration 8000 / 40000: loss 1.521730\n",
      "iteration 9000 / 40000: loss 1.444342\n",
      "iteration 10000 / 40000: loss 1.402721\n",
      "iteration 11000 / 40000: loss 1.461449\n",
      "iteration 12000 / 40000: loss 1.443917\n",
      "iteration 13000 / 40000: loss 1.424894\n",
      "iteration 14000 / 40000: loss 1.426459\n",
      "iteration 15000 / 40000: loss 1.545114\n",
      "iteration 16000 / 40000: loss 1.460897\n",
      "iteration 17000 / 40000: loss 1.519473\n",
      "iteration 18000 / 40000: loss 1.368885\n",
      "iteration 19000 / 40000: loss 1.559249\n",
      "iteration 20000 / 40000: loss 1.343180\n",
      "iteration 21000 / 40000: loss 1.464218\n",
      "iteration 22000 / 40000: loss 1.441385\n",
      "iteration 23000 / 40000: loss 1.397893\n",
      "iteration 24000 / 40000: loss 1.418281\n",
      "iteration 25000 / 40000: loss 1.458311\n",
      "iteration 26000 / 40000: loss 1.463112\n",
      "iteration 27000 / 40000: loss 1.399056\n",
      "iteration 28000 / 40000: loss 1.404042\n",
      "iteration 29000 / 40000: loss 1.583268\n",
      "iteration 30000 / 40000: loss 1.460516\n",
      "iteration 31000 / 40000: loss 1.515021\n",
      "iteration 32000 / 40000: loss 1.498458\n",
      "iteration 33000 / 40000: loss 1.471079\n",
      "iteration 34000 / 40000: loss 1.467656\n",
      "iteration 35000 / 40000: loss 1.561187\n",
      "iteration 36000 / 40000: loss 1.380047\n",
      "iteration 37000 / 40000: loss 1.417908\n",
      "iteration 38000 / 40000: loss 1.454317\n",
      "iteration 39000 / 40000: loss 1.385520\n",
      "Train accuracy:  0.49597560975609756\n",
      "Loss:  1.4642064424490895\n",
      "Test accuracy:  0.488\n",
      " \n",
      "The 3 th training\n",
      "iteration 0 / 40000: loss 2.302597\n",
      "iteration 1000 / 40000: loss 1.733198\n",
      "iteration 2000 / 40000: loss 1.749808\n",
      "iteration 3000 / 40000: loss 1.575775\n",
      "iteration 4000 / 40000: loss 1.638081\n",
      "iteration 5000 / 40000: loss 1.554467\n",
      "iteration 6000 / 40000: loss 1.617692\n",
      "iteration 7000 / 40000: loss 1.478425\n",
      "iteration 8000 / 40000: loss 1.405959\n",
      "iteration 9000 / 40000: loss 1.480907\n",
      "iteration 10000 / 40000: loss 1.479820\n",
      "iteration 11000 / 40000: loss 1.460813\n",
      "iteration 12000 / 40000: loss 1.410885\n",
      "iteration 13000 / 40000: loss 1.568199\n",
      "iteration 14000 / 40000: loss 1.654831\n",
      "iteration 15000 / 40000: loss 1.512360\n",
      "iteration 16000 / 40000: loss 1.455967\n",
      "iteration 17000 / 40000: loss 1.441556\n",
      "iteration 18000 / 40000: loss 1.447823\n",
      "iteration 19000 / 40000: loss 1.415564\n",
      "iteration 20000 / 40000: loss 1.485877\n",
      "iteration 21000 / 40000: loss 1.418426\n",
      "iteration 22000 / 40000: loss 1.442696\n",
      "iteration 23000 / 40000: loss 1.561324\n",
      "iteration 24000 / 40000: loss 1.368154\n",
      "iteration 25000 / 40000: loss 1.469612\n",
      "iteration 26000 / 40000: loss 1.501382\n",
      "iteration 27000 / 40000: loss 1.438338\n",
      "iteration 28000 / 40000: loss 1.524983\n",
      "iteration 29000 / 40000: loss 1.490778\n",
      "iteration 30000 / 40000: loss 1.408786\n",
      "iteration 31000 / 40000: loss 1.477682\n",
      "iteration 32000 / 40000: loss 1.392887\n",
      "iteration 33000 / 40000: loss 1.582655\n",
      "iteration 34000 / 40000: loss 1.601793\n",
      "iteration 35000 / 40000: loss 1.434014\n",
      "iteration 36000 / 40000: loss 1.541267\n",
      "iteration 37000 / 40000: loss 1.370808\n",
      "iteration 38000 / 40000: loss 1.509980\n",
      "iteration 39000 / 40000: loss 1.462114\n",
      "Train accuracy:  0.48908536585365847\n",
      "Loss:  1.4672067468646453\n",
      "Test accuracy:  0.492\n",
      " \n",
      "The 4 th training\n",
      "iteration 0 / 40000: loss 2.302609\n",
      "iteration 1000 / 40000: loss 1.747509\n",
      "iteration 2000 / 40000: loss 1.650375\n",
      "iteration 3000 / 40000: loss 1.610660\n",
      "iteration 4000 / 40000: loss 1.370823\n",
      "iteration 5000 / 40000: loss 1.535285\n",
      "iteration 6000 / 40000: loss 1.573377\n",
      "iteration 7000 / 40000: loss 1.491965\n",
      "iteration 8000 / 40000: loss 1.488357\n",
      "iteration 9000 / 40000: loss 1.507775\n",
      "iteration 10000 / 40000: loss 1.565011\n",
      "iteration 11000 / 40000: loss 1.578690\n",
      "iteration 12000 / 40000: loss 1.451617\n",
      "iteration 13000 / 40000: loss 1.538551\n",
      "iteration 14000 / 40000: loss 1.372040\n",
      "iteration 15000 / 40000: loss 1.508399\n",
      "iteration 16000 / 40000: loss 1.427937\n",
      "iteration 17000 / 40000: loss 1.464338\n",
      "iteration 18000 / 40000: loss 1.444043\n",
      "iteration 19000 / 40000: loss 1.444614\n",
      "iteration 20000 / 40000: loss 1.372774\n",
      "iteration 21000 / 40000: loss 1.559457\n",
      "iteration 22000 / 40000: loss 1.606352\n",
      "iteration 23000 / 40000: loss 1.458068\n",
      "iteration 24000 / 40000: loss 1.354437\n",
      "iteration 25000 / 40000: loss 1.530953\n",
      "iteration 26000 / 40000: loss 1.653153\n",
      "iteration 27000 / 40000: loss 1.481034\n",
      "iteration 28000 / 40000: loss 1.569670\n",
      "iteration 29000 / 40000: loss 1.513520\n",
      "iteration 30000 / 40000: loss 1.538034\n",
      "iteration 31000 / 40000: loss 1.465228\n",
      "iteration 32000 / 40000: loss 1.491598\n",
      "iteration 33000 / 40000: loss 1.435035\n",
      "iteration 34000 / 40000: loss 1.479728\n",
      "iteration 35000 / 40000: loss 1.319800\n",
      "iteration 36000 / 40000: loss 1.507829\n",
      "iteration 37000 / 40000: loss 1.505058\n",
      "iteration 38000 / 40000: loss 1.360882\n",
      "iteration 39000 / 40000: loss 1.426110\n",
      "Train accuracy:  0.4987499999999999\n",
      "Loss:  1.4648595021154154\n",
      "Test accuracy:  0.469\n",
      " \n",
      "The 5 th training\n",
      "iteration 0 / 40000: loss 2.302601\n",
      "iteration 1000 / 40000: loss 1.786353\n",
      "iteration 2000 / 40000: loss 1.661675\n",
      "iteration 3000 / 40000: loss 1.514730\n",
      "iteration 4000 / 40000: loss 1.395049\n",
      "iteration 5000 / 40000: loss 1.458639\n",
      "iteration 6000 / 40000: loss 1.423250\n",
      "iteration 7000 / 40000: loss 1.438077\n",
      "iteration 8000 / 40000: loss 1.541601\n",
      "iteration 9000 / 40000: loss 1.294211\n",
      "iteration 10000 / 40000: loss 1.393559\n",
      "iteration 11000 / 40000: loss 1.385909\n",
      "iteration 12000 / 40000: loss 1.372667\n",
      "iteration 13000 / 40000: loss 1.471280\n",
      "iteration 14000 / 40000: loss 1.408462\n",
      "iteration 15000 / 40000: loss 1.401390\n",
      "iteration 16000 / 40000: loss 1.475263\n",
      "iteration 17000 / 40000: loss 1.422011\n",
      "iteration 18000 / 40000: loss 1.495241\n",
      "iteration 19000 / 40000: loss 1.493108\n",
      "iteration 20000 / 40000: loss 1.576538\n",
      "iteration 21000 / 40000: loss 1.627019\n",
      "iteration 22000 / 40000: loss 1.463300\n",
      "iteration 23000 / 40000: loss 1.491386\n",
      "iteration 24000 / 40000: loss 1.399101\n",
      "iteration 25000 / 40000: loss 1.569759\n",
      "iteration 26000 / 40000: loss 1.487582\n",
      "iteration 27000 / 40000: loss 1.542252\n",
      "iteration 28000 / 40000: loss 1.292814\n",
      "iteration 29000 / 40000: loss 1.351289\n",
      "iteration 30000 / 40000: loss 1.504043\n",
      "iteration 31000 / 40000: loss 1.427524\n",
      "iteration 32000 / 40000: loss 1.486866\n",
      "iteration 33000 / 40000: loss 1.488971\n",
      "iteration 34000 / 40000: loss 1.450308\n",
      "iteration 35000 / 40000: loss 1.465388\n",
      "iteration 36000 / 40000: loss 1.393242\n",
      "iteration 37000 / 40000: loss 1.617872\n",
      "iteration 38000 / 40000: loss 1.448990\n",
      "iteration 39000 / 40000: loss 1.403264\n",
      "Train accuracy:  0.49716463414634143\n",
      "Loss:  1.4559846962203218\n",
      "Test accuracy:  0.497\n",
      " \n",
      "The 6 th training\n",
      "iteration 0 / 40000: loss 2.302591\n",
      "iteration 1000 / 40000: loss 1.695601\n",
      "iteration 2000 / 40000: loss 1.581984\n",
      "iteration 3000 / 40000: loss 1.505182\n",
      "iteration 4000 / 40000: loss 1.451109\n",
      "iteration 5000 / 40000: loss 1.567178\n",
      "iteration 6000 / 40000: loss 1.417683\n",
      "iteration 7000 / 40000: loss 1.391419\n",
      "iteration 8000 / 40000: loss 1.413710\n",
      "iteration 9000 / 40000: loss 1.430072\n",
      "iteration 10000 / 40000: loss 1.545324\n",
      "iteration 11000 / 40000: loss 1.533362\n",
      "iteration 12000 / 40000: loss 1.495930\n",
      "iteration 13000 / 40000: loss 1.581552\n",
      "iteration 14000 / 40000: loss 1.552932\n",
      "iteration 15000 / 40000: loss 1.443688\n",
      "iteration 16000 / 40000: loss 1.420796\n",
      "iteration 17000 / 40000: loss 1.558287\n",
      "iteration 18000 / 40000: loss 1.408290\n",
      "iteration 19000 / 40000: loss 1.385512\n",
      "iteration 20000 / 40000: loss 1.400953\n",
      "iteration 21000 / 40000: loss 1.531086\n",
      "iteration 22000 / 40000: loss 1.549682\n",
      "iteration 23000 / 40000: loss 1.543616\n",
      "iteration 24000 / 40000: loss 1.521954\n",
      "iteration 25000 / 40000: loss 1.481997\n",
      "iteration 26000 / 40000: loss 1.366191\n",
      "iteration 27000 / 40000: loss 1.499012\n",
      "iteration 28000 / 40000: loss 1.440449\n",
      "iteration 29000 / 40000: loss 1.556073\n",
      "iteration 30000 / 40000: loss 1.451144\n",
      "iteration 31000 / 40000: loss 1.525979\n",
      "iteration 32000 / 40000: loss 1.647313\n",
      "iteration 33000 / 40000: loss 1.486346\n",
      "iteration 34000 / 40000: loss 1.409201\n",
      "iteration 35000 / 40000: loss 1.460256\n",
      "iteration 36000 / 40000: loss 1.494533\n",
      "iteration 37000 / 40000: loss 1.409124\n",
      "iteration 38000 / 40000: loss 1.506273\n",
      "iteration 39000 / 40000: loss 1.461050\n",
      "Train accuracy:  0.4902134146341464\n",
      "Loss:  1.467218650433355\n",
      "Test accuracy:  0.489\n",
      " \n",
      "The 7 th training\n",
      "iteration 0 / 40000: loss 2.302609\n",
      "iteration 1000 / 40000: loss 1.694093\n",
      "iteration 2000 / 40000: loss 1.629856\n",
      "iteration 3000 / 40000: loss 1.560051\n",
      "iteration 4000 / 40000: loss 1.599899\n",
      "iteration 5000 / 40000: loss 1.617627\n",
      "iteration 6000 / 40000: loss 1.492764\n",
      "iteration 7000 / 40000: loss 1.468683\n",
      "iteration 8000 / 40000: loss 1.601077\n",
      "iteration 9000 / 40000: loss 1.442667\n",
      "iteration 10000 / 40000: loss 1.559300\n",
      "iteration 11000 / 40000: loss 1.409664\n",
      "iteration 12000 / 40000: loss 1.451615\n",
      "iteration 13000 / 40000: loss 1.556613\n",
      "iteration 14000 / 40000: loss 1.463284\n",
      "iteration 15000 / 40000: loss 1.549135\n",
      "iteration 16000 / 40000: loss 1.606367\n",
      "iteration 17000 / 40000: loss 1.559538\n",
      "iteration 18000 / 40000: loss 1.534319\n",
      "iteration 19000 / 40000: loss 1.484340\n",
      "iteration 20000 / 40000: loss 1.595674\n",
      "iteration 21000 / 40000: loss 1.462957\n",
      "iteration 22000 / 40000: loss 1.376154\n",
      "iteration 23000 / 40000: loss 1.479054\n",
      "iteration 24000 / 40000: loss 1.478297\n",
      "iteration 25000 / 40000: loss 1.540764\n",
      "iteration 26000 / 40000: loss 1.628835\n",
      "iteration 27000 / 40000: loss 1.302130\n",
      "iteration 28000 / 40000: loss 1.541464\n",
      "iteration 29000 / 40000: loss 1.512612\n",
      "iteration 30000 / 40000: loss 1.443501\n",
      "iteration 31000 / 40000: loss 1.497257\n",
      "iteration 32000 / 40000: loss 1.482868\n",
      "iteration 33000 / 40000: loss 1.415945\n",
      "iteration 34000 / 40000: loss 1.461058\n",
      "iteration 35000 / 40000: loss 1.478200\n",
      "iteration 36000 / 40000: loss 1.483950\n",
      "iteration 37000 / 40000: loss 1.495112\n",
      "iteration 38000 / 40000: loss 1.479405\n",
      "iteration 39000 / 40000: loss 1.391346\n",
      "Train accuracy:  0.49338414634146344\n",
      "Loss:  1.4733418644322234\n",
      "Test accuracy:  0.484\n",
      " \n",
      "The 8 th training\n",
      "iteration 0 / 40000: loss 2.302607\n",
      "iteration 1000 / 40000: loss 1.673455\n",
      "iteration 2000 / 40000: loss 1.597775\n",
      "iteration 3000 / 40000: loss 1.615200\n",
      "iteration 4000 / 40000: loss 1.505195\n",
      "iteration 5000 / 40000: loss 1.495364\n",
      "iteration 6000 / 40000: loss 1.460493\n",
      "iteration 7000 / 40000: loss 1.517773\n",
      "iteration 8000 / 40000: loss 1.527792\n",
      "iteration 9000 / 40000: loss 1.508222\n",
      "iteration 10000 / 40000: loss 1.559631\n",
      "iteration 11000 / 40000: loss 1.452214\n",
      "iteration 12000 / 40000: loss 1.583452\n",
      "iteration 13000 / 40000: loss 1.490031\n",
      "iteration 14000 / 40000: loss 1.458741\n",
      "iteration 15000 / 40000: loss 1.504667\n",
      "iteration 16000 / 40000: loss 1.512785\n",
      "iteration 17000 / 40000: loss 1.506104\n",
      "iteration 18000 / 40000: loss 1.487093\n",
      "iteration 19000 / 40000: loss 1.525613\n",
      "iteration 20000 / 40000: loss 1.507609\n",
      "iteration 21000 / 40000: loss 1.499611\n",
      "iteration 22000 / 40000: loss 1.436898\n",
      "iteration 23000 / 40000: loss 1.616195\n",
      "iteration 24000 / 40000: loss 1.473786\n",
      "iteration 25000 / 40000: loss 1.455186\n",
      "iteration 26000 / 40000: loss 1.585568\n",
      "iteration 27000 / 40000: loss 1.560382\n",
      "iteration 28000 / 40000: loss 1.519234\n",
      "iteration 29000 / 40000: loss 1.515433\n",
      "iteration 30000 / 40000: loss 1.483813\n",
      "iteration 31000 / 40000: loss 1.516105\n",
      "iteration 32000 / 40000: loss 1.571525\n",
      "iteration 33000 / 40000: loss 1.523242\n",
      "iteration 34000 / 40000: loss 1.634625\n",
      "iteration 35000 / 40000: loss 1.508296\n",
      "iteration 36000 / 40000: loss 1.375565\n",
      "iteration 37000 / 40000: loss 1.503708\n",
      "iteration 38000 / 40000: loss 1.418796\n",
      "iteration 39000 / 40000: loss 1.510266\n",
      "Train accuracy:  0.4923780487804878\n",
      "Loss:  1.4819404991621083\n",
      "Test accuracy:  0.464\n",
      " \n",
      "The 9 th training\n",
      "iteration 0 / 40000: loss 2.302605\n",
      "iteration 1000 / 40000: loss 1.708103\n",
      "iteration 2000 / 40000: loss 1.578410\n",
      "iteration 3000 / 40000: loss 1.556022\n",
      "iteration 4000 / 40000: loss 1.442351\n",
      "iteration 5000 / 40000: loss 1.487006\n",
      "iteration 6000 / 40000: loss 1.457520\n",
      "iteration 7000 / 40000: loss 1.505628\n",
      "iteration 8000 / 40000: loss 1.575665\n",
      "iteration 9000 / 40000: loss 1.573411\n",
      "iteration 10000 / 40000: loss 1.500122\n",
      "iteration 11000 / 40000: loss 1.521747\n",
      "iteration 12000 / 40000: loss 1.466506\n",
      "iteration 13000 / 40000: loss 1.404681\n",
      "iteration 14000 / 40000: loss 1.446950\n",
      "iteration 15000 / 40000: loss 1.484175\n",
      "iteration 16000 / 40000: loss 1.380572\n",
      "iteration 17000 / 40000: loss 1.509261\n",
      "iteration 18000 / 40000: loss 1.493025\n",
      "iteration 19000 / 40000: loss 1.428412\n",
      "iteration 20000 / 40000: loss 1.395995\n",
      "iteration 21000 / 40000: loss 1.520992\n",
      "iteration 22000 / 40000: loss 1.504012\n",
      "iteration 23000 / 40000: loss 1.482837\n",
      "iteration 24000 / 40000: loss 1.472498\n",
      "iteration 25000 / 40000: loss 1.522603\n",
      "iteration 26000 / 40000: loss 1.332336\n",
      "iteration 27000 / 40000: loss 1.427072\n",
      "iteration 28000 / 40000: loss 1.487919\n",
      "iteration 29000 / 40000: loss 1.541481\n",
      "iteration 30000 / 40000: loss 1.481532\n",
      "iteration 31000 / 40000: loss 1.382826\n",
      "iteration 32000 / 40000: loss 1.438324\n",
      "iteration 33000 / 40000: loss 1.599760\n",
      "iteration 34000 / 40000: loss 1.467200\n",
      "iteration 35000 / 40000: loss 1.594266\n",
      "iteration 36000 / 40000: loss 1.533649\n",
      "iteration 37000 / 40000: loss 1.504784\n",
      "iteration 38000 / 40000: loss 1.429831\n",
      "iteration 39000 / 40000: loss 1.426523\n",
      "Train accuracy:  0.4849390243902439\n",
      "Loss:  1.4763649886753927\n",
      "Test accuracy:  0.463\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Training use SGD \n",
    "\n",
    "#Network Settings\n",
    "\n",
    "input_size = 16 * 16 * 3\n",
    "hidden_size = 30\n",
    "num_classes = 10\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "#np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparams\n",
    "batch_size = 200\n",
    "learning_rate = 5e-4\n",
    "reg = 0.1\n",
    "\n",
    "test_acc_list_bp = []\n",
    "train_acc_list_bp = []\n",
    "loss_list_bp = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('The %d th training' % i)\n",
    "    net_bp = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "    stats_bp = net_bp.train_bp(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=40000, batch_size=batch_size,\n",
    "                    learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                    reg=reg, verbose=True, print_every = 1000)\n",
    "\n",
    "    loss_avg = np.mean(stats_bp['loss_history'][-5000:])\n",
    "    train_acc_avg = np.mean(stats_bp['train_acc_history'][-5000:])\n",
    "    print('Train accuracy: ', train_acc_avg)\n",
    "    print('Loss: ', loss_avg)\n",
    "    \n",
    "    test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "    print('Test accuracy: ', test_acc)\n",
    "    print(' ')\n",
    "    \n",
    "    loss_list_bp.append(loss_avg)\n",
    "    train_acc_list_bp.append(train_acc_avg)\n",
    "    test_acc_list_bp.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8UDQl+B3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22005589278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4708206179647354\n",
      "0.48996951219512197\n"
     ]
    }
   ],
   "source": [
    "plt.plot(stats_bp['loss_history'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(stats_bp['loss_history'][-5000:]))\n",
    "print(np.mean(stats_bp['train_acc_history'][-5000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAE/CAYAAAAOkIE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlBI6ERERERGRkPp/DqW9OseYjBAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b083c37ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:\n",
      "bp  1.468\n",
      "alter  1.430\n",
      "alter_adv  1.485\n",
      " \n",
      "Mean train accuracy:\n",
      "bp  0.493\n",
      "alter  0.528\n",
      "alter_adv  0.517\n",
      " \n",
      "Mean test accuracy:\n",
      "bp  0.479\n",
      "alter  0.485\n",
      "alter_adv  0.486\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(np.ones(10), loss_list_bp, 'o')\n",
    "plt.plot(np.ones(10)*2, loss_list_alter, 'o')\n",
    "plt.plot(np.ones(10)*3, loss_list_alter_adv, 'o')\n",
    "plt.title('loss')\n",
    "plt.legend(['bp', 'alter', 'adv'])\n",
    "#plt.show()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(np.ones(10), train_acc_list_bp, 'o')\n",
    "plt.plot(np.ones(10)*2, train_acc_list_alter, 'o')\n",
    "plt.plot(np.ones(10)*3, train_acc_list_alter_adv, 'o')\n",
    "plt.title('Train accuracy')\n",
    "plt.legend(['bp', 'alter', 'adv'])\n",
    "#plt.show()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(np.ones(10), test_acc_list_bp, 'o')\n",
    "plt.plot(np.ones(10)*2, test_acc_list_alter, 'o')\n",
    "plt.plot(np.ones(10)*3, test_acc_list_alter_adv, 'o')\n",
    "plt.title('Test accuracy')\n",
    "plt.legend(['bp', 'alter', 'adv'])\n",
    "plt.show()\n",
    "\n",
    "print('Mean loss:')\n",
    "print('bp %6.3f' % np.mean(loss_list_bp))\n",
    "print('alter %6.3f' % np.mean(loss_list_alter))\n",
    "print('alter_adv %6.3f' % np.mean(loss_list_alter_adv))\n",
    "print(' ')\n",
    "print('Mean train accuracy:')\n",
    "print('bp %6.3f' % np.mean(train_acc_list_bp))\n",
    "print('alter %6.3f' % np.mean(train_acc_list_alter))\n",
    "print('alter_adv %6.3f' % np.mean(train_acc_list_alter_adv))\n",
    "print(' ')\n",
    "print('Mean test accuracy:')\n",
    "print('bp %6.3f' % np.mean(test_acc_list_bp))\n",
    "print('alter %6.3f' % np.mean(test_acc_list_alter))\n",
    "print('alter_adv %6.3f' % np.mean(test_acc_list_alter_adv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
