{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Author:  SYH & SXK\n",
    "#DATE:    2018.9.4\n",
    "#TASK:    NN with SA\n",
    "\n",
    "#Restructured from CS231n assignment1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from head.data_utils import load_CIFAR10\n",
    "\n",
    "\n",
    "#Load Data\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network Settings\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment of alternative training using SGD and SA\n",
    "\n",
    "In SA, W1 and b1 are fixed.\n",
    "Multiply T during exploration is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 2.302747\n",
      "iteration 100 / 2000: loss 2.151115\n",
      "iteration 200 / 2000: loss 1.956375\n",
      "iteration 300 / 2000: loss 1.795100\n",
      "iteration 400 / 2000: loss 1.650096\n",
      "iteration 500 / 2000: loss 1.788248\n",
      "iteration 600 / 2000: loss 1.632312\n",
      "iteration 700 / 2000: loss 1.725397\n",
      "iteration 800 / 2000: loss 1.546313\n",
      "iteration 900 / 2000: loss 1.617557\n",
      "iteration 1000 / 2000: loss 1.605412\n",
      "iteration 1100 / 2000: loss 1.411051\n",
      "iteration 1200 / 2000: loss 1.647157\n",
      "iteration 1300 / 2000: loss 1.290863\n",
      "iteration 1400 / 2000: loss 1.594383\n",
      "iteration 1500 / 2000: loss 1.539742\n",
      "iteration 1600 / 2000: loss 1.498160\n",
      "iteration 1700 / 2000: loss 1.507776\n",
      "iteration 1800 / 2000: loss 1.503318\n",
      "iteration 1900 / 2000: loss 1.481487\n",
      "Test accuracy:  0.491\n"
     ]
    }
   ],
   "source": [
    "#Training use SGD \n",
    "#Test set accuracy around 50%\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparams\n",
    "batch_size = 236\n",
    "learning_rate = 5e-4\n",
    "reg = 0.1\n",
    "\n",
    "net_bp = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_bp = net_bp.train_bp(X_train, y_train, X_val, y_val,\n",
    "                num_iters=2000, batch_size=batch_size,\n",
    "                learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                reg=reg, verbose=True)\n",
    "\n",
    "test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Documents\\1资料\\Brainmatrix\\模拟退火\\CBICR_contest\\Source\\head\\neural_net.py:175: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.473559\n",
      "Reject: 0   Accept: 1\n",
      "iteration 100 / 1000: loss 1.291111\n",
      "Reject: 44   Accept: 57\n",
      "iteration 200 / 1000: loss 1.318650\n",
      "Reject: 100   Accept: 101\n",
      "iteration 300 / 1000: loss 1.403399\n",
      "Reject: 165   Accept: 136\n",
      "iteration 400 / 1000: loss 1.307760\n",
      "Reject: 237   Accept: 164\n",
      "iteration 500 / 1000: loss 1.297264\n",
      "Reject: 322   Accept: 179\n",
      "iteration 600 / 1000: loss 1.270016\n",
      "Reject: 418   Accept: 183\n",
      "iteration 700 / 1000: loss 1.200882\n",
      "Reject: 517   Accept: 184\n",
      "iteration 800 / 1000: loss 1.200882\n",
      "Reject: 617   Accept: 184\n",
      "iteration 900 / 1000: loss 1.200882\n",
      "Reject: 717   Accept: 184\n",
      "Test accuracy:  0.492\n"
     ]
    }
   ],
   "source": [
    "#Training hyperparameters\n",
    "batch_size = 236\n",
    "step_len = 0.001\n",
    "reg = 0.1\n",
    "\n",
    "#net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_sa = net_bp.train_sa(X_train, y_train, X_val, y_val,\n",
    "        num_iters=1000, batch_size=batch_size, step_len = step_len,\n",
    "        reg=reg, T_max = 0.1, T_min = 0.005, verbose=True)\n",
    "\n",
    "test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.424908\n",
      "iteration 100 / 1000: loss 1.423528\n",
      "iteration 200 / 1000: loss 1.352588\n",
      "iteration 300 / 1000: loss 1.447310\n",
      "iteration 400 / 1000: loss 1.420273\n",
      "iteration 500 / 1000: loss 1.513390\n",
      "iteration 600 / 1000: loss 1.486947\n",
      "iteration 700 / 1000: loss 1.446780\n",
      "iteration 800 / 1000: loss 1.401696\n",
      "iteration 900 / 1000: loss 1.433270\n",
      "Test accuracy:  0.511\n"
     ]
    }
   ],
   "source": [
    "#net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_bp2 = net_bp.train_bp(X_train, y_train, X_val, y_val,\n",
    "                num_iters=1000, batch_size=batch_size,\n",
    "                learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                reg=reg, verbose=True)\n",
    "\n",
    "test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Documents\\1资料\\Brainmatrix\\模拟退火\\CBICR_contest\\Source\\head\\neural_net.py:175: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.313374\n",
      "Reject: 0   Accept: 1\n",
      "iteration 100 / 1000: loss 1.373535\n",
      "Reject: 36   Accept: 65\n",
      "iteration 200 / 1000: loss 1.400866\n",
      "Reject: 87   Accept: 114\n",
      "iteration 300 / 1000: loss 1.297289\n",
      "Reject: 165   Accept: 136\n",
      "iteration 400 / 1000: loss 1.306708\n",
      "Reject: 245   Accept: 156\n",
      "iteration 500 / 1000: loss 1.140807\n",
      "Reject: 336   Accept: 165\n",
      "iteration 600 / 1000: loss 1.140807\n",
      "Reject: 436   Accept: 165\n",
      "iteration 700 / 1000: loss 1.140807\n",
      "Reject: 536   Accept: 165\n",
      "iteration 800 / 1000: loss 1.140807\n",
      "Reject: 636   Accept: 165\n",
      "iteration 900 / 1000: loss 1.140807\n",
      "Reject: 736   Accept: 165\n",
      "Test accuracy:  0.512\n"
     ]
    }
   ],
   "source": [
    "#Training hyperparameters\n",
    "batch_size = 236\n",
    "step_len = 0.001\n",
    "reg = 0.1\n",
    "\n",
    "#net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_sa = net_bp.train_sa(X_train, y_train, X_val, y_val,\n",
    "        num_iters=1000, batch_size=batch_size, step_len = step_len,\n",
    "        reg=reg, T_max = 0.1, T_min = 0.005, verbose=True)\n",
    "\n",
    "test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.543214\n",
      "iteration 100 / 1000: loss 1.374317\n",
      "iteration 200 / 1000: loss 1.421916\n",
      "iteration 300 / 1000: loss 1.394191\n",
      "iteration 400 / 1000: loss 1.469426\n",
      "iteration 500 / 1000: loss 1.297635\n",
      "iteration 600 / 1000: loss 1.307569\n",
      "iteration 700 / 1000: loss 1.275755\n",
      "iteration 800 / 1000: loss 1.401675\n",
      "iteration 900 / 1000: loss 1.286454\n",
      "Test accuracy:  0.508\n"
     ]
    }
   ],
   "source": [
    "#net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_bp2 = net_bp.train_bp(X_train, y_train, X_val, y_val,\n",
    "                num_iters=1000, batch_size=batch_size,\n",
    "                learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                reg=reg, verbose=True)\n",
    "\n",
    "test_acc = (net_bp.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010020800039624187\n",
      "0.061793816715325044\n",
      "0.0007240720681355025\n",
      "0.01172892439159337\n",
      "0.001073910838031649\n",
      "0.015721310429476804\n",
      "0.00017228960479820647\n",
      "0.005593125225003647\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(stats_bp['params']['W1']).max())\n",
    "print(np.abs(stats_bp['params']['W2']).max())\n",
    "print(np.abs(stats_bp['params']['b1']).max())\n",
    "print(np.abs(stats_bp['params']['b2']).max())\n",
    "\n",
    "print(np.abs(stats_bp['params']['W1']).mean())\n",
    "print(np.abs(stats_bp['params']['W2']).mean())\n",
    "print(np.abs(stats_bp['params']['b1']).mean())\n",
    "print(np.abs(stats_bp['params']['b2']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Documents\\1资料\\Brainmatrix\\模拟退火\\CBICR_contest\\Source\\head\\neural_net.py:175: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp((loss_past - loss_new) / T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 99 / 3000: loss 2.979058\n",
      "Reject: 4   Accept: 96\n",
      "iteration 199 / 3000: loss 3.683077\n",
      "Reject: 17   Accept: 183\n",
      "iteration 299 / 3000: loss 4.331144\n",
      "Reject: 33   Accept: 267\n",
      "iteration 399 / 3000: loss 5.530801\n",
      "Reject: 64   Accept: 336\n",
      "iteration 499 / 3000: loss 5.777062\n",
      "Reject: 114   Accept: 386\n",
      "iteration 599 / 3000: loss 6.366315\n",
      "Reject: 170   Accept: 430\n",
      "iteration 699 / 3000: loss 7.005427\n",
      "Reject: 226   Accept: 474\n",
      "iteration 799 / 3000: loss 7.263470\n",
      "Reject: 286   Accept: 514\n",
      "iteration 899 / 3000: loss 7.427310\n",
      "Reject: 360   Accept: 540\n",
      "iteration 999 / 3000: loss 7.586750\n",
      "Reject: 433   Accept: 567\n",
      "iteration 1099 / 3000: loss 7.858143\n",
      "Reject: 508   Accept: 592\n",
      "iteration 1199 / 3000: loss 7.843852\n",
      "Reject: 601   Accept: 599\n",
      "iteration 1299 / 3000: loss 7.825159\n",
      "Reject: 683   Accept: 617\n",
      "iteration 1399 / 3000: loss 7.972960\n",
      "Reject: 782   Accept: 618\n",
      "iteration 1499 / 3000: loss 7.972960\n",
      "Reject: 882   Accept: 618\n",
      "iteration 1599 / 3000: loss 8.393579\n",
      "Reject: 975   Accept: 625\n",
      "iteration 1699 / 3000: loss 8.144628\n",
      "Reject: 1073   Accept: 627\n",
      "iteration 1799 / 3000: loss 7.981861\n",
      "Reject: 1172   Accept: 628\n",
      "iteration 1899 / 3000: loss 7.981861\n",
      "Reject: 1272   Accept: 628\n",
      "iteration 1999 / 3000: loss 7.981861\n",
      "Reject: 1372   Accept: 628\n",
      "iteration 2099 / 3000: loss 7.981861\n",
      "Reject: 1472   Accept: 628\n",
      "iteration 2199 / 3000: loss 7.981861\n",
      "Reject: 1572   Accept: 628\n",
      "iteration 2299 / 3000: loss 7.981861\n",
      "Reject: 1672   Accept: 628\n",
      "iteration 2399 / 3000: loss 8.018058\n",
      "Reject: 1770   Accept: 630\n",
      "iteration 2499 / 3000: loss 7.973063\n",
      "Reject: 1869   Accept: 631\n",
      "iteration 2599 / 3000: loss 7.973063\n",
      "Reject: 1969   Accept: 631\n",
      "iteration 2699 / 3000: loss 7.973063\n",
      "Reject: 2069   Accept: 631\n",
      "iteration 2799 / 3000: loss 7.973063\n",
      "Reject: 2169   Accept: 631\n",
      "iteration 2899 / 3000: loss 7.973063\n",
      "Reject: 2269   Accept: 631\n",
      "iteration 2999 / 3000: loss 7.973063\n",
      "Reject: 2369   Accept: 631\n",
      "Test accuracy:  0.101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training use SA\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparameters\n",
    "batch_size = 200\n",
    "step_len = 0.001\n",
    "reg = 0.1\n",
    "\n",
    "net_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_sa = net_sa.train_sa(X_train, y_train, X_val, y_val,\n",
    "        num_iters=3000, batch_size=batch_size, step_len = step_len,\n",
    "        reg=reg, T_max = 0.5, T_min = 0.005, verbose=True)\n",
    "\n",
    "test_acc = (net_sa.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06763021757869667\n",
      "0.044185503375490265\n",
      "0.03387870266059838\n",
      "0.027565011235918223\n",
      "0.011571865638164666\n",
      "0.011692327013066071\n",
      "0.01070166995606491\n",
      "0.010970357185687686\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(stats_sa['params']['W1']).max())\n",
    "print(np.abs(stats_sa['params']['W2']).max())\n",
    "print(np.abs(stats_sa['params']['b1']).max())\n",
    "print(np.abs(stats_sa['params']['b2']).max())\n",
    "\n",
    "print(np.abs(stats_sa['params']['W1']).mean())\n",
    "print(np.abs(stats_sa['params']['W2']).mean())\n",
    "print(np.abs(stats_sa['params']['b1']).mean())\n",
    "print(np.abs(stats_sa['params']['b2']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200: loss 2.302747\n",
      "iteration 100 / 200: loss 2.302751\n",
      "Accept: 0   Reject: 200\n",
      "Test accuracy:  0.097\n"
     ]
    }
   ],
   "source": [
    "#Experiment by SYH\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "#Training hyperparams\n",
    "batch_size = 236\n",
    "step_len = 5e-4\n",
    "reg = 0.1\n",
    "\n",
    "net_test = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_test = net_test.train_test(X_train, y_train, X_val, y_val,\n",
    "                num_iters=200, batch_size=batch_size,\n",
    "                step_len = step_len, sigma = 1,\n",
    "                reg=reg, verbose=True)\n",
    "\n",
    "test_acc = (net_test.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_sgd sgd 1 / 2, iteration 0 / 6000: loss 2.30, train_acc 0.18\n",
      "it_sgd sgd 1 / 2, iteration 100 / 6000: loss 1.90, train_acc 0.30\n",
      "it_sgd sgd 1 / 2, iteration 200 / 6000: loss 1.67, train_acc 0.43\n",
      "it_sgd sgd 1 / 2, iteration 300 / 6000: loss 1.67, train_acc 0.46\n",
      "it_sgd sgd 1 / 2, iteration 400 / 6000: loss 1.67, train_acc 0.49\n",
      "it_sgd sgd 1 / 2, iteration 500 / 6000: loss 1.56, train_acc 0.52\n",
      "it_sgd sgd 1 / 2, iteration 600 / 6000: loss 1.52, train_acc 0.54\n",
      "it_sgd sgd 1 / 2, iteration 700 / 6000: loss 1.39, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 800 / 6000: loss 1.52, train_acc 0.55\n",
      "it_sgd sgd 1 / 2, iteration 900 / 6000: loss 1.57, train_acc 0.54\n",
      "it_sgd sgd 1 / 2, iteration 1000 / 6000: loss 1.50, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 1100 / 6000: loss 1.42, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 1200 / 6000: loss 1.37, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 1300 / 6000: loss 1.36, train_acc 0.61\n",
      "it_sgd sgd 1 / 2, iteration 1400 / 6000: loss 1.45, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 1500 / 6000: loss 1.39, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 1600 / 6000: loss 1.50, train_acc 0.57\n",
      "it_sgd sgd 1 / 2, iteration 1700 / 6000: loss 1.46, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 1800 / 6000: loss 1.56, train_acc 0.61\n",
      "it_sgd sgd 1 / 2, iteration 1900 / 6000: loss 1.41, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 2000 / 6000: loss 1.32, train_acc 0.65\n",
      "it_sgd sgd 1 / 2, iteration 2100 / 6000: loss 1.48, train_acc 0.53\n",
      "it_sgd sgd 1 / 2, iteration 2200 / 6000: loss 1.50, train_acc 0.57\n",
      "it_sgd sgd 1 / 2, iteration 2300 / 6000: loss 1.42, train_acc 0.57\n",
      "it_sgd sgd 1 / 2, iteration 2400 / 6000: loss 1.28, train_acc 0.65\n",
      "it_sgd sgd 1 / 2, iteration 2500 / 6000: loss 1.24, train_acc 0.63\n",
      "it_sgd sgd 1 / 2, iteration 2600 / 6000: loss 1.43, train_acc 0.56\n",
      "it_sgd sgd 1 / 2, iteration 2700 / 6000: loss 1.30, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 2800 / 6000: loss 1.41, train_acc 0.56\n",
      "it_sgd sgd 1 / 2, iteration 2900 / 6000: loss 1.33, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 3000 / 6000: loss 1.37, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 3100 / 6000: loss 1.26, train_acc 0.64\n",
      "it_sgd sgd 1 / 2, iteration 3200 / 6000: loss 1.26, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 3300 / 6000: loss 1.28, train_acc 0.60\n",
      "it_sgd sgd 1 / 2, iteration 3400 / 6000: loss 1.27, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 3500 / 6000: loss 1.28, train_acc 0.66\n",
      "it_sgd sgd 1 / 2, iteration 3600 / 6000: loss 1.32, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 3700 / 6000: loss 1.27, train_acc 0.64\n",
      "it_sgd sgd 1 / 2, iteration 3800 / 6000: loss 1.16, train_acc 0.66\n",
      "it_sgd sgd 1 / 2, iteration 3900 / 6000: loss 1.28, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 4000 / 6000: loss 1.24, train_acc 0.65\n",
      "it_sgd sgd 1 / 2, iteration 4100 / 6000: loss 1.22, train_acc 0.67\n",
      "it_sgd sgd 1 / 2, iteration 4200 / 6000: loss 1.22, train_acc 0.66\n",
      "it_sgd sgd 1 / 2, iteration 4300 / 6000: loss 1.19, train_acc 0.64\n",
      "it_sgd sgd 1 / 2, iteration 4400 / 6000: loss 1.21, train_acc 0.60\n",
      "it_sgd sgd 1 / 2, iteration 4500 / 6000: loss 1.21, train_acc 0.62\n",
      "it_sgd sgd 1 / 2, iteration 4600 / 6000: loss 1.25, train_acc 0.67\n",
      "it_sgd sgd 1 / 2, iteration 4700 / 6000: loss 1.27, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 4800 / 6000: loss 1.22, train_acc 0.65\n",
      "it_sgd sgd 1 / 2, iteration 4900 / 6000: loss 1.13, train_acc 0.65\n",
      "it_sgd sgd 1 / 2, iteration 5000 / 6000: loss 1.20, train_acc 0.61\n",
      "it_sgd sgd 1 / 2, iteration 5100 / 6000: loss 1.34, train_acc 0.56\n",
      "it_sgd sgd 1 / 2, iteration 5200 / 6000: loss 1.24, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 5300 / 6000: loss 1.15, train_acc 0.64\n",
      "it_sgd sgd 1 / 2, iteration 5400 / 6000: loss 1.36, train_acc 0.57\n",
      "it_sgd sgd 1 / 2, iteration 5500 / 6000: loss 1.30, train_acc 0.61\n",
      "it_sgd sgd 1 / 2, iteration 5600 / 6000: loss 1.22, train_acc 0.64\n",
      "it_sgd sgd 1 / 2, iteration 5700 / 6000: loss 1.25, train_acc 0.59\n",
      "it_sgd sgd 1 / 2, iteration 5800 / 6000: loss 1.36, train_acc 0.58\n",
      "it_sgd sgd 1 / 2, iteration 5900 / 6000: loss 1.26, train_acc 0.61\n",
      "it_sgd sa 1 / 2, iteration 0 / 500: loss 1.12, train_acc 0.62\n",
      "it_sgd sa 1 / 2, iteration 100 / 500: loss 1.14, train_acc 0.65\n",
      "it_sgd sa 1 / 2, iteration 200 / 500: loss 1.14, train_acc 0.64\n",
      "it_sgd sa 1 / 2, iteration 300 / 500: loss 1.15, train_acc 0.65\n",
      "it_sgd sa 1 / 2, iteration 400 / 500: loss 1.15, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 0 / 6000: loss 1.13, train_acc 0.71\n",
      "it_sgd sgd 2 / 2, iteration 100 / 6000: loss 1.62, train_acc 0.48\n",
      "it_sgd sgd 2 / 2, iteration 200 / 6000: loss 1.28, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 300 / 6000: loss 1.41, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 400 / 6000: loss 1.26, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 500 / 6000: loss 1.34, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 600 / 6000: loss 1.55, train_acc 0.57\n",
      "it_sgd sgd 2 / 2, iteration 700 / 6000: loss 1.17, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 800 / 6000: loss 1.45, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 900 / 6000: loss 1.29, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 1000 / 6000: loss 1.27, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 1100 / 6000: loss 1.44, train_acc 0.62\n",
      "it_sgd sgd 2 / 2, iteration 1200 / 6000: loss 1.23, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 1300 / 6000: loss 1.31, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 1400 / 6000: loss 1.24, train_acc 0.73\n",
      "it_sgd sgd 2 / 2, iteration 1500 / 6000: loss 1.38, train_acc 0.66\n",
      "it_sgd sgd 2 / 2, iteration 1600 / 6000: loss 1.27, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 1700 / 6000: loss 1.25, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 1800 / 6000: loss 1.29, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 1900 / 6000: loss 1.38, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 2000 / 6000: loss 1.42, train_acc 0.60\n",
      "it_sgd sgd 2 / 2, iteration 2100 / 6000: loss 1.31, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 2200 / 6000: loss 1.21, train_acc 0.71\n",
      "it_sgd sgd 2 / 2, iteration 2300 / 6000: loss 1.42, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 2400 / 6000: loss 1.22, train_acc 0.71\n",
      "it_sgd sgd 2 / 2, iteration 2500 / 6000: loss 1.32, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 2600 / 6000: loss 1.24, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 2700 / 6000: loss 1.25, train_acc 0.72\n",
      "it_sgd sgd 2 / 2, iteration 2800 / 6000: loss 1.26, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 2900 / 6000: loss 1.45, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 3000 / 6000: loss 1.26, train_acc 0.66\n",
      "it_sgd sgd 2 / 2, iteration 3100 / 6000: loss 1.13, train_acc 0.73\n",
      "it_sgd sgd 2 / 2, iteration 3200 / 6000: loss 1.41, train_acc 0.64\n",
      "it_sgd sgd 2 / 2, iteration 3300 / 6000: loss 1.25, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 3400 / 6000: loss 1.34, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 3500 / 6000: loss 1.26, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 3600 / 6000: loss 1.21, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 3700 / 6000: loss 1.15, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 3800 / 6000: loss 1.15, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 3900 / 6000: loss 1.15, train_acc 0.71\n",
      "it_sgd sgd 2 / 2, iteration 4000 / 6000: loss 1.19, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 4100 / 6000: loss 1.27, train_acc 0.64\n",
      "it_sgd sgd 2 / 2, iteration 4200 / 6000: loss 1.16, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 4300 / 6000: loss 1.13, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 4400 / 6000: loss 1.18, train_acc 0.71\n",
      "it_sgd sgd 2 / 2, iteration 4500 / 6000: loss 1.12, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 4600 / 6000: loss 1.19, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 4700 / 6000: loss 1.12, train_acc 0.67\n",
      "it_sgd sgd 2 / 2, iteration 4800 / 6000: loss 1.11, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 4900 / 6000: loss 1.19, train_acc 0.66\n",
      "it_sgd sgd 2 / 2, iteration 5000 / 6000: loss 1.28, train_acc 0.62\n",
      "it_sgd sgd 2 / 2, iteration 5100 / 6000: loss 1.15, train_acc 0.68\n",
      "it_sgd sgd 2 / 2, iteration 5200 / 6000: loss 1.26, train_acc 0.66\n",
      "it_sgd sgd 2 / 2, iteration 5300 / 6000: loss 1.15, train_acc 0.64\n",
      "it_sgd sgd 2 / 2, iteration 5400 / 6000: loss 1.16, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 5500 / 6000: loss 1.11, train_acc 0.70\n",
      "it_sgd sgd 2 / 2, iteration 5600 / 6000: loss 1.14, train_acc 0.65\n",
      "it_sgd sgd 2 / 2, iteration 5700 / 6000: loss 1.14, train_acc 0.64\n",
      "it_sgd sgd 2 / 2, iteration 5800 / 6000: loss 1.19, train_acc 0.69\n",
      "it_sgd sgd 2 / 2, iteration 5900 / 6000: loss 1.21, train_acc 0.66\n",
      "Test accuracy:  0.503\n"
     ]
    }
   ],
   "source": [
    "#Experiment by SXK\n",
    "\n",
    "from head.neural_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#Training hyperparams\n",
    "#batch_size = 200\n",
    "#step_len = 5e-4\n",
    "#reg = 0.1\n",
    "\n",
    "net_bp_sa = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_bp_sa = net_bp_sa.train_bp_sa(X_train, y_train, X_val, y_val, num_iters_per_sgd = 6000, \n",
    "                                    num_sgds = 2, verbose=True)\n",
    "\n",
    "test_acc = (net_bp_sa.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_sgd sgd 1 / 1, iteration 0 / 12000: loss 2.30, train_acc 0.18\n",
      "it_sgd sgd 1 / 1, iteration 100 / 12000: loss 1.90, train_acc 0.30\n",
      "it_sgd sgd 1 / 1, iteration 200 / 12000: loss 1.67, train_acc 0.43\n",
      "it_sgd sgd 1 / 1, iteration 300 / 12000: loss 1.67, train_acc 0.47\n",
      "it_sgd sgd 1 / 1, iteration 400 / 12000: loss 1.67, train_acc 0.50\n",
      "it_sgd sgd 1 / 1, iteration 500 / 12000: loss 1.56, train_acc 0.52\n",
      "it_sgd sgd 1 / 1, iteration 600 / 12000: loss 1.52, train_acc 0.53\n",
      "it_sgd sgd 1 / 1, iteration 700 / 12000: loss 1.39, train_acc 0.57\n",
      "it_sgd sgd 1 / 1, iteration 800 / 12000: loss 1.49, train_acc 0.54\n",
      "it_sgd sgd 1 / 1, iteration 900 / 12000: loss 1.57, train_acc 0.48\n",
      "it_sgd sgd 1 / 1, iteration 1000 / 12000: loss 1.49, train_acc 0.55\n",
      "it_sgd sgd 1 / 1, iteration 1100 / 12000: loss 1.41, train_acc 0.58\n",
      "it_sgd sgd 1 / 1, iteration 1200 / 12000: loss 1.38, train_acc 0.60\n",
      "it_sgd sgd 1 / 1, iteration 1300 / 12000: loss 1.35, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 1400 / 12000: loss 1.46, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 1500 / 12000: loss 1.40, train_acc 0.61\n",
      "it_sgd sgd 1 / 1, iteration 1600 / 12000: loss 1.51, train_acc 0.56\n",
      "it_sgd sgd 1 / 1, iteration 1700 / 12000: loss 1.45, train_acc 0.58\n",
      "it_sgd sgd 1 / 1, iteration 1800 / 12000: loss 1.57, train_acc 0.58\n",
      "it_sgd sgd 1 / 1, iteration 1900 / 12000: loss 1.43, train_acc 0.60\n",
      "it_sgd sgd 1 / 1, iteration 2000 / 12000: loss 1.30, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 2100 / 12000: loss 1.50, train_acc 0.52\n",
      "it_sgd sgd 1 / 1, iteration 2200 / 12000: loss 1.51, train_acc 0.58\n",
      "it_sgd sgd 1 / 1, iteration 2300 / 12000: loss 1.47, train_acc 0.57\n",
      "it_sgd sgd 1 / 1, iteration 2400 / 12000: loss 1.33, train_acc 0.61\n",
      "it_sgd sgd 1 / 1, iteration 2500 / 12000: loss 1.24, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 2600 / 12000: loss 1.43, train_acc 0.58\n",
      "it_sgd sgd 1 / 1, iteration 2700 / 12000: loss 1.32, train_acc 0.64\n",
      "it_sgd sgd 1 / 1, iteration 2800 / 12000: loss 1.43, train_acc 0.59\n",
      "it_sgd sgd 1 / 1, iteration 2900 / 12000: loss 1.36, train_acc 0.59\n",
      "it_sgd sgd 1 / 1, iteration 3000 / 12000: loss 1.45, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 3100 / 12000: loss 1.24, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 3200 / 12000: loss 1.32, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 3300 / 12000: loss 1.31, train_acc 0.60\n",
      "it_sgd sgd 1 / 1, iteration 3400 / 12000: loss 1.31, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 3500 / 12000: loss 1.35, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 3600 / 12000: loss 1.36, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 3700 / 12000: loss 1.31, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 3800 / 12000: loss 1.21, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 3900 / 12000: loss 1.47, train_acc 0.61\n",
      "it_sgd sgd 1 / 1, iteration 4000 / 12000: loss 1.27, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 4100 / 12000: loss 1.23, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 4200 / 12000: loss 1.32, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 4300 / 12000: loss 1.24, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 4400 / 12000: loss 1.31, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 4500 / 12000: loss 1.22, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 4600 / 12000: loss 1.27, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 4700 / 12000: loss 1.30, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 4800 / 12000: loss 1.24, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 4900 / 12000: loss 1.20, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 5000 / 12000: loss 1.27, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 5100 / 12000: loss 1.33, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 5200 / 12000: loss 1.29, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 5300 / 12000: loss 1.14, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 5400 / 12000: loss 1.39, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 5500 / 12000: loss 1.32, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 5600 / 12000: loss 1.28, train_acc 0.70\n",
      "it_sgd sgd 1 / 1, iteration 5700 / 12000: loss 1.26, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 5800 / 12000: loss 1.37, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 5900 / 12000: loss 1.28, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 6000 / 12000: loss 1.20, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 6100 / 12000: loss 1.19, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 6200 / 12000: loss 1.25, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 6300 / 12000: loss 1.31, train_acc 0.64\n",
      "it_sgd sgd 1 / 1, iteration 6400 / 12000: loss 1.22, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 6500 / 12000: loss 1.29, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 6600 / 12000: loss 1.33, train_acc 0.60\n",
      "it_sgd sgd 1 / 1, iteration 6700 / 12000: loss 1.25, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 6800 / 12000: loss 1.26, train_acc 0.70\n",
      "it_sgd sgd 1 / 1, iteration 6900 / 12000: loss 1.26, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 7000 / 12000: loss 1.23, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 7100 / 12000: loss 1.11, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 7200 / 12000: loss 1.26, train_acc 0.70\n",
      "it_sgd sgd 1 / 1, iteration 7300 / 12000: loss 1.19, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 7400 / 12000: loss 1.16, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 7500 / 12000: loss 1.44, train_acc 0.61\n",
      "it_sgd sgd 1 / 1, iteration 7600 / 12000: loss 1.21, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 7700 / 12000: loss 1.14, train_acc 0.72\n",
      "it_sgd sgd 1 / 1, iteration 7800 / 12000: loss 1.20, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 7900 / 12000: loss 1.18, train_acc 0.73\n",
      "it_sgd sgd 1 / 1, iteration 8000 / 12000: loss 1.15, train_acc 0.72\n",
      "it_sgd sgd 1 / 1, iteration 8100 / 12000: loss 1.22, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 8200 / 12000: loss 1.07, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 8300 / 12000: loss 1.17, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 8400 / 12000: loss 1.16, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 8500 / 12000: loss 1.18, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 8600 / 12000: loss 1.22, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 8700 / 12000: loss 1.15, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 8800 / 12000: loss 1.23, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 8900 / 12000: loss 1.18, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 9000 / 12000: loss 1.20, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 9100 / 12000: loss 1.30, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 9200 / 12000: loss 1.26, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 9300 / 12000: loss 1.15, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 9400 / 12000: loss 1.06, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 9500 / 12000: loss 1.20, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 9600 / 12000: loss 1.11, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 9700 / 12000: loss 1.12, train_acc 0.70\n",
      "it_sgd sgd 1 / 1, iteration 9800 / 12000: loss 1.08, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 9900 / 12000: loss 1.27, train_acc 0.64\n",
      "it_sgd sgd 1 / 1, iteration 10000 / 12000: loss 1.08, train_acc 0.71\n",
      "it_sgd sgd 1 / 1, iteration 10100 / 12000: loss 1.09, train_acc 0.72\n",
      "it_sgd sgd 1 / 1, iteration 10200 / 12000: loss 1.15, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 10300 / 12000: loss 1.15, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 10400 / 12000: loss 1.19, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 10500 / 12000: loss 1.16, train_acc 0.72\n",
      "it_sgd sgd 1 / 1, iteration 10600 / 12000: loss 1.14, train_acc 0.69\n",
      "it_sgd sgd 1 / 1, iteration 10700 / 12000: loss 1.20, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 10800 / 12000: loss 1.23, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 10900 / 12000: loss 1.20, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 11000 / 12000: loss 1.09, train_acc 0.70\n",
      "it_sgd sgd 1 / 1, iteration 11100 / 12000: loss 1.25, train_acc 0.62\n",
      "it_sgd sgd 1 / 1, iteration 11200 / 12000: loss 1.22, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 11300 / 12000: loss 1.24, train_acc 0.65\n",
      "it_sgd sgd 1 / 1, iteration 11400 / 12000: loss 1.21, train_acc 0.66\n",
      "it_sgd sgd 1 / 1, iteration 11500 / 12000: loss 1.20, train_acc 0.64\n",
      "it_sgd sgd 1 / 1, iteration 11600 / 12000: loss 1.11, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 11700 / 12000: loss 1.11, train_acc 0.67\n",
      "it_sgd sgd 1 / 1, iteration 11800 / 12000: loss 1.15, train_acc 0.68\n",
      "it_sgd sgd 1 / 1, iteration 11900 / 12000: loss 1.10, train_acc 0.65\n",
      "Test accuracy:  0.503\n"
     ]
    }
   ],
   "source": [
    "#from head.neural_net import TwoLayerNet\n",
    "np.random.seed(0)\n",
    "net_bp = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_bp = net_bp.train_bp_sa(X_train, y_train, X_val, y_val, num_iters_per_sgd = 12000,\n",
    "                              num_sgds = 1, if_sa = False, verbose=True)\n",
    "\n",
    "test_acc = (net_bp_sa.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats_bp['train_acc_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxCGs3BmGYeKQ/wfsyQPGBK7MwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253a5040c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253a50405c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stats_bp_sa['loss_history'][8000:])\n",
    "plt.plot(stats_bp['loss_history'][8000:])\n",
    "plt.title('Loss')\n",
    "plt.legend(['with sa', 'without sa'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(stats_bp_sa['train_acc_history'])\n",
    "plt.plot(stats_bp['train_acc_history'])\n",
    "plt.title('train accuracy')\n",
    "plt.legend(['with sa', 'without sa'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from head.neural_net import *\n",
    "import numpy as np\n",
    "model = FourLayerConvNet_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.3025851003923394\n",
      "Initial loss (with regularization):  2.327206920411432\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3, 32, 32)\n",
      "(Iteration 1 / 9800) loss: 2.302584\n",
      "(Epoch 0 / 10) train acc: 0.110000; val_acc: 0.102000\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 21 / 9800) loss: 2.302614\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 41 / 9800) loss: 2.302586\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 61 / 9800) loss: 2.302530\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 81 / 9800) loss: 2.302544\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 101 / 9800) loss: 2.302616\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 121 / 9800) loss: 2.302600\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 141 / 9800) loss: 2.302648\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 161 / 9800) loss: 2.302565\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 181 / 9800) loss: 2.302648\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 201 / 9800) loss: 2.302615\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 221 / 9800) loss: 2.302561\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 241 / 9800) loss: 2.302527\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 261 / 9800) loss: 2.302537\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 281 / 9800) loss: 2.302703\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 301 / 9800) loss: 2.302538\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 321 / 9800) loss: 2.302696\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 341 / 9800) loss: 2.302597\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 361 / 9800) loss: 2.302525\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 381 / 9800) loss: 2.302716\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 401 / 9800) loss: 2.302534\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 421 / 9800) loss: 2.302720\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 441 / 9800) loss: 2.302605\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 461 / 9800) loss: 2.302469\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 481 / 9800) loss: 2.302464\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 501 / 9800) loss: 2.302518\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 521 / 9800) loss: 2.302521\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 541 / 9800) loss: 2.302625\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 561 / 9800) loss: 2.302815\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 581 / 9800) loss: 2.302593\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 601 / 9800) loss: 2.302623\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 621 / 9800) loss: 2.302686\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 641 / 9800) loss: 2.302698\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 661 / 9800) loss: 2.302664\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 681 / 9800) loss: 2.302579\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 701 / 9800) loss: 2.302589\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 721 / 9800) loss: 2.302563\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 741 / 9800) loss: 2.302398\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 761 / 9800) loss: 2.302562\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 781 / 9800) loss: 2.302548\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 801 / 9800) loss: 2.302578\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 821 / 9800) loss: 2.302809\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 841 / 9800) loss: 2.302516\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 861 / 9800) loss: 2.302454\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 881 / 9800) loss: 2.302529\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 901 / 9800) loss: 2.302612\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 921 / 9800) loss: 2.302833\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 941 / 9800) loss: 2.302833\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(Iteration 961 / 9800) loss: 2.302567\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n",
      "(50, 3, 32, 32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b562bd6f1f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 },\n\u001b[1;32m     27\u001b[0m                 verbose=True, print_every=20)\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\utils\\solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[1;31m# Maybe print training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\utils\\solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[1;31m# Compute loss and gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\neural_net.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[1;31m# Backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mdaffine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b4'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maffine_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mdmax_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_backward_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaffine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mdX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmax_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_relu_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\utils\\fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_backward_fast\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax_pool_backward_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'im2col'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax_pool_backward_im2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized method \"%s\"'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\utils\\fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_backward_im2col\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mdx_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_cols_argmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx_cols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout_reshaped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     dx = col2im_indices(dx_cols, (N * C, 1, H, W), pool_height, pool_width,\n\u001b[0;32m--> 276\u001b[0;31m                 padding=0, stride=stride)\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\CBICR_contest\\Source\\head\\utils\\im2col.py\u001b[0m in \u001b[0;36mcol2im_indices\u001b[0;34m(cols, x_shape, field_height, field_width, padding, stride)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcols_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfield_height\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfield_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mcols_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols_reshaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_padded\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from head.neural_net import *\n",
    "import numpy as np\n",
    "from head.utils.solver import Solver\n",
    "\n",
    "model = FourLayerConvNet_fast()\n",
    "\n",
    "#num_train = 100\n",
    "X_train = X_train.reshape((X_train.shape[0], 3, 32, 32))\n",
    "X_val = X_val.reshape((X_val.shape[0], 3, 32, 32))\n",
    "X_test = X_test.reshape((X_test.shape[0], 3, 32, 32))\n",
    "\n",
    "data = {\n",
    "  'X_train': X_train,\n",
    "  'y_train': y_train,\n",
    "  'X_val': X_val,\n",
    "  'y_val': y_val,\n",
    "  'X_test': X_test,\n",
    "  'y_test': y_test\n",
    "}\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
